{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aleksandrmorozov123/deep-learning-for-nlp?scriptVersionId=156466025\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-25T17:41:02.659207Z","iopub.execute_input":"2023-12-25T17:41:02.660453Z","iopub.status.idle":"2023-12-25T17:41:03.129766Z","shell.execute_reply.started":"2023-12-25T17:41:02.660413Z","shell.execute_reply":"2023-12-25T17:41:03.128533Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Checking statistics of the Corpus**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport pandas as pd\nreviews = pd.read_csv ('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\nreviews.info()","metadata":{"execution":{"iopub.status.busy":"2023-12-25T17:41:03.132633Z","iopub.execute_input":"2023-12-25T17:41:03.133538Z","iopub.status.idle":"2023-12-25T17:41:04.841625Z","shell.execute_reply.started":"2023-12-25T17:41:03.133493Z","shell.execute_reply":"2023-12-25T17:41:04.840067Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 50000 entries, 0 to 49999\nData columns (total 2 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   review     50000 non-null  object\n 1   sentiment  50000 non-null  object\ndtypes: object(2)\nmemory usage: 781.4+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"# comparing the text of two selected reviews\nprint (repr(reviews.iloc[3344]['review'][0:300]))\nprint (repr(reviews.iloc[23909]['review'][0:300]))","metadata":{"execution":{"iopub.status.busy":"2023-12-25T17:41:04.843325Z","iopub.execute_input":"2023-12-25T17:41:04.844057Z","iopub.status.idle":"2023-12-25T17:41:04.852376Z","shell.execute_reply.started":"2023-12-25T17:41:04.844015Z","shell.execute_reply":"2023-12-25T17:41:04.851199Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"'The first time I saw this \"film\" I loved it. When I was 11, I was more interested in the music and dancing. As I\\'ve grown older, I\\'ve become more interested in the acting as well. While the first half is just a retrospective of Michael\\'s career (from the Jackson 5 up to \"Bad\"), it was still entertai'\n'...now please move on because that\\'s getting on my nerves.<br /><br />Seriously, the man behind brilliant pieces like \"My Own Private Idaho\" and \"To Die For\" (and others not so brilliant movies, i.e. the unnecessary \"Psycho\" remake) started an experimental phase with \"Gerry\", which reached its peak '\n","output_type":"stream"}]},{"cell_type":"code","source":"# ignore spaces after the stop words\nimport re\nreviews [\"paragraphs\"] = reviews [\"review\"].map (lambda text: re.split ('[.?!]\\s*\\n', text))\nreviews ['number_of_paragraphs'] = reviews [\"paragraphs\"].map (len)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T17:41:04.85403Z","iopub.execute_input":"2023-12-25T17:41:04.855067Z","iopub.status.idle":"2023-12-25T17:41:06.157306Z","shell.execute_reply.started":"2023-12-25T17:41:04.85502Z","shell.execute_reply":"2023-12-25T17:41:06.15602Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"**Preparations**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport sklearn\nimport spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom spacy.lang.de.stop_words import STOP_WORDS\n\ntfidf_text_vectorizer = TfidfVectorizer(stop_words=list(STOP_WORDS))\nvectors_text = tfidf_text_vectorizer.fit_transform (reviews ['review'])\nvectors_text.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-25T17:41:06.160047Z","iopub.execute_input":"2023-12-25T17:41:06.160439Z","iopub.status.idle":"2023-12-25T17:41:29.351978Z","shell.execute_reply.started":"2023-12-25T17:41:06.160406Z","shell.execute_reply":"2023-12-25T17:41:29.350772Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(50000, 101758)"},"metadata":{}}]},{"cell_type":"code","source":"# flatten the paragraphs keeping the sentiment\nparagraph_df = pd.DataFrame ([{'review': paragraph, 'sentiment': sentiment}\n                             for paragraphs, sentiment in \\\n                             zip (reviews ['paragraphs'], reviews ['sentiment'])\n                             for paragraph in paragraphs if paragraph])\ntfidf_para_vectorizer = TfidfVectorizer(stop_words=list(STOP_WORDS))\ntfidf_para_vectors = tfidf_para_vectorizer.fit_transform (paragraph_df ['review'])\ntfidf_para_vectors.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-25T17:41:29.353558Z","iopub.execute_input":"2023-12-25T17:41:29.354244Z","iopub.status.idle":"2023-12-25T17:41:45.749675Z","shell.execute_reply.started":"2023-12-25T17:41:29.35421Z","shell.execute_reply":"2023-12-25T17:41:45.74848Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(50000, 101758)"},"metadata":{}}]},{"cell_type":"markdown","source":"**Nonnegative matrix factorization** - $ V \\approx W \\cdot H $","metadata":{}},{"cell_type":"code","source":"# import required library\nfrom sklearn.decomposition import NMF\n\nnmf_text_model = NMF (n_components = 10, random_state = 42)\nW_text_matrix = nmf_text_model.fit_transform (vectors_text)\nH_text_matrix = nmf_text_model.components_\n\n# define a function for outputtin a summary\ndef display_topics (model, features, no_top_words=5):\n    for topic, word_vector in enumerate (nmf_text_model.components_):\n        total = word_vector.sum ()\n        largest = word_vector.argsort ()[::-1]  # invert sort order\n        print (\"\\ntopic %02d\" % topic)\n        for i in range (0, no_top_words):\n            print (\"  %s (%2.2f)\" % (features [largest [i]],\n                                    word_vector [largest[i]] * 100.0/total))\n            \n# calling the function\ndisplay_topics (nmf_text_model, tfidf_text_vectorizer.get_feature_names_out())","metadata":{"execution":{"iopub.status.busy":"2023-12-25T17:41:45.750844Z","iopub.execute_input":"2023-12-25T17:41:45.751177Z","iopub.status.idle":"2023-12-25T17:42:26.765494Z","shell.execute_reply.started":"2023-12-25T17:41:45.751136Z","shell.execute_reply":"2023-12-25T17:42:26.76432Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1665: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\ntopic 00\n  the (8.01)\n  of (1.84)\n  to (0.47)\n  from (0.43)\n  on (0.41)\n\ntopic 01\n  br (22.11)\n  10 (0.50)\n  some (0.26)\n  no (0.25)\n  here (0.24)\n\ntopic 02\n  to (2.22)\n  they (1.33)\n  that (1.24)\n  have (0.75)\n  show (0.73)\n\ntopic 03\n  he (2.65)\n  his (2.18)\n  to (0.96)\n  him (0.92)\n  the (0.89)\n\ntopic 04\n  film (4.47)\n  is (2.06)\n  this (1.92)\n  films (0.97)\n  to (0.96)\n\ntopic 05\n  movie (6.44)\n  this (3.26)\n  is (1.94)\n  bad (1.40)\n  movies (1.32)\n\ntopic 06\n  and (3.04)\n  of (1.22)\n  is (1.17)\n  are (0.60)\n  as (0.60)\n\ntopic 07\n  you (6.92)\n  if (2.34)\n  your (1.59)\n  don (0.90)\n  watch (0.88)\n\ntopic 08\n  she (4.31)\n  is (1.02)\n  the (0.79)\n  to (0.76)\n  and (0.56)\n\ntopic 09\n  it (6.17)\n  and (1.74)\n  but (1.19)\n  my (1.07)\n  the (0.97)\n","output_type":"stream"}]},{"cell_type":"code","source":"# normalizing topics\nW_text_matrix.sum (axis=0)/W_text_matrix.sum()*100.0","metadata":{"execution":{"iopub.status.busy":"2023-12-25T17:42:26.76673Z","iopub.execute_input":"2023-12-25T17:42:26.767035Z","iopub.status.idle":"2023-12-25T17:42:26.775849Z","shell.execute_reply.started":"2023-12-25T17:42:26.767009Z","shell.execute_reply":"2023-12-25T17:42:26.774794Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array([17.79233196, 11.90474076,  9.18845062,  8.77492193,  8.10281665,\n        9.50987968,  9.76571393,  8.26599463,  5.81269207, 10.88245777])"},"metadata":{}}]},{"cell_type":"markdown","source":"**Create a topic model for paragraphs using NMF**","metadata":{}},{"cell_type":"code","source":"nmf_para_model = NMF (n_components = 10, random_state = 42)\nW_para_matrix = nmf_para_model.fit_transform (tfidf_para_vectors)\nH_para_matrix = nmf_para_model.components_\n\ndisplay_topics (nmf_para_model, tfidf_para_vectorizer.get_feature_names_out ())","metadata":{"execution":{"iopub.status.busy":"2023-12-25T17:48:58.421732Z","iopub.execute_input":"2023-12-25T17:48:58.422251Z","iopub.status.idle":"2023-12-25T17:49:38.605052Z","shell.execute_reply.started":"2023-12-25T17:48:58.422216Z","shell.execute_reply":"2023-12-25T17:49:38.603879Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1665: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\ntopic 00\n  the (8.01)\n  of (1.84)\n  to (0.47)\n  from (0.43)\n  on (0.41)\n\ntopic 01\n  br (22.11)\n  10 (0.50)\n  some (0.26)\n  no (0.25)\n  here (0.24)\n\ntopic 02\n  to (2.22)\n  they (1.33)\n  that (1.24)\n  have (0.75)\n  show (0.73)\n\ntopic 03\n  he (2.65)\n  his (2.18)\n  to (0.96)\n  him (0.92)\n  the (0.89)\n\ntopic 04\n  film (4.47)\n  is (2.06)\n  this (1.92)\n  films (0.97)\n  to (0.96)\n\ntopic 05\n  movie (6.44)\n  this (3.26)\n  is (1.94)\n  bad (1.40)\n  movies (1.32)\n\ntopic 06\n  and (3.04)\n  of (1.22)\n  is (1.17)\n  are (0.60)\n  as (0.60)\n\ntopic 07\n  you (6.92)\n  if (2.34)\n  your (1.59)\n  don (0.90)\n  watch (0.88)\n\ntopic 08\n  she (4.31)\n  is (1.02)\n  the (0.79)\n  to (0.76)\n  and (0.56)\n\ntopic 09\n  it (6.17)\n  and (1.74)\n  but (1.19)\n  my (1.07)\n  the (0.97)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Latent semantic analysis with singular value decomposition** - any $ m \\times n $ matrix V can be decomposed as follows\n$V = U \\cdot \\Sigma \\cdot V^* $","metadata":{}},{"cell_type":"code","source":"# import required module\nfrom sklearn.decomposition import TruncatedSVD\n\nsvd_para_model = TruncatedSVD (n_components = 10, random_state = 42)\nW_svd_para_matrix = svd_para_model.fit_transform (tfidf_para_vectors)\nH_svd_para_matrix = svd_para_model.components_\n\ndisplay_topics (svd_para_model, tfidf_para_vectorizer.get_feature_names_out ())","metadata":{"execution":{"iopub.status.busy":"2023-12-25T17:55:44.662308Z","iopub.execute_input":"2023-12-25T17:55:44.662717Z","iopub.status.idle":"2023-12-25T17:55:48.0545Z","shell.execute_reply.started":"2023-12-25T17:55:44.662687Z","shell.execute_reply":"2023-12-25T17:55:48.053233Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"\ntopic 00\n  the (8.01)\n  of (1.84)\n  to (0.47)\n  from (0.43)\n  on (0.41)\n\ntopic 01\n  br (22.11)\n  10 (0.50)\n  some (0.26)\n  no (0.25)\n  here (0.24)\n\ntopic 02\n  to (2.22)\n  they (1.33)\n  that (1.24)\n  have (0.75)\n  show (0.73)\n\ntopic 03\n  he (2.65)\n  his (2.18)\n  to (0.96)\n  him (0.92)\n  the (0.89)\n\ntopic 04\n  film (4.47)\n  is (2.06)\n  this (1.92)\n  films (0.97)\n  to (0.96)\n\ntopic 05\n  movie (6.44)\n  this (3.26)\n  is (1.94)\n  bad (1.40)\n  movies (1.32)\n\ntopic 06\n  and (3.04)\n  of (1.22)\n  is (1.17)\n  are (0.60)\n  as (0.60)\n\ntopic 07\n  you (6.92)\n  if (2.34)\n  your (1.59)\n  don (0.90)\n  watch (0.88)\n\ntopic 08\n  she (4.31)\n  is (1.02)\n  the (0.79)\n  to (0.76)\n  and (0.56)\n\ntopic 09\n  it (6.17)\n  and (1.74)\n  but (1.19)\n  my (1.07)\n  the (0.97)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Latent Dirichlet Allocation**","metadata":{}},{"cell_type":"code","source":"# import required modules\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\ncount_para_vectorizer = CountVectorizer (stop_words=list(STOP_WORDS))\ncount_para_vectors = count_para_vectorizer.fit_transform (paragraph_df ['review'])\n\nlda_para_model = LatentDirichletAllocation (n_components = 10, random_state = 42)\nW_lda_para_matrix = lda_para_model.fit_transform (count_para_vectors)\nH_lda_para_matrix = lda_para_model.components_\n\ndisplay_topics (lda_para_model, tfidf_para_vectorizer.get_feature_names_out ())","metadata":{"execution":{"iopub.status.busy":"2023-12-25T18:18:16.826557Z","iopub.execute_input":"2023-12-25T18:18:16.827004Z","iopub.status.idle":"2023-12-25T18:18:54.149805Z","shell.execute_reply.started":"2023-12-25T18:18:16.82697Z","shell.execute_reply":"2023-12-25T18:18:54.148182Z"},"trusted":true},"execution_count":16,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m count_para_vectors \u001b[38;5;241m=\u001b[39m count_para_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform (paragraph_df [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      8\u001b[0m lda_para_model \u001b[38;5;241m=\u001b[39m LatentDirichletAllocation (n_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m W_lda_para_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mlda_para_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcount_para_vectors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m H_lda_para_matrix \u001b[38;5;241m=\u001b[39m lda_para_model\u001b[38;5;241m.\u001b[39mcomponents_\n\u001b[1;32m     12\u001b[0m display_topics (lda_para_model, tfidf_para_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out ())\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_lda.py:668\u001b[0m, in \u001b[0;36mLatentDirichletAllocation.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_em_step(\n\u001b[1;32m    661\u001b[0m             X[idx_slice, :],\n\u001b[1;32m    662\u001b[0m             total_samples\u001b[38;5;241m=\u001b[39mn_samples,\n\u001b[1;32m    663\u001b[0m             batch_update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    664\u001b[0m             parallel\u001b[38;5;241m=\u001b[39mparallel,\n\u001b[1;32m    665\u001b[0m         )\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;66;03m# batch update\u001b[39;00m\n\u001b[0;32m--> 668\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_em_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_update\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;66;03m# check perplexity\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_every \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m evaluate_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_lda.py:517\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._em_step\u001b[0;34m(self, X, total_samples, batch_update, parallel)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"EM update for 1 iteration.\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03mupdate `_component` by batch VB or online VB.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;124;03m    Unnormalized document topic distribution.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;66;03m# E-step\u001b[39;00m\n\u001b[0;32m--> 517\u001b[0m _, suff_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcal_sstats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# M-step\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_update:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_lda.py:460\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._e_step\u001b[0;34m(self, X, cal_sstats, random_init, parallel)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parallel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 460\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_update_doc_distribution\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp_dirichlet_component_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc_topic_prior_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_doc_update_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean_change_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcal_sstats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx_slice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen_even_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;66;03m# merge result\u001b[39;00m\n\u001b[1;32m    474\u001b[0m doc_topics, sstats_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_lda.py:141\u001b[0m, in \u001b[0;36m_update_doc_distribution\u001b[0;34m(X, exp_topic_word_distr, doc_topic_prior, max_doc_update_iter, mean_change_tol, cal_sstats, random_state)\u001b[0m\n\u001b[1;32m    139\u001b[0m doc_topic_d \u001b[38;5;241m=\u001b[39m exp_doc_topic_d \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(cnts \u001b[38;5;241m/\u001b[39m norm_phi, exp_topic_word_d\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Note: adds doc_topic_prior to doc_topic_d, in-place.\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m \u001b[43mdirichlet_expectation_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_topic_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_topic_prior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_doc_topic_d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mean_change(last_d, doc_topic_d) \u001b[38;5;241m<\u001b[39m mean_change_tol:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# visualizing LDA results\nimport pyLDAvis.sklearn\n\nlda_display = pyLDAvis.sklearn.prepare (lda_para_model, count_para_vectors,\n                                       count_para_vectorizer, sort_topics = False)\npyLDAvis.display (lda_display)","metadata":{},"execution_count":null,"outputs":[]}]}