{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":796646,"sourceType":"datasetVersion","datasetId":19136},{"sourceId":1304644,"sourceType":"datasetVersion","datasetId":754810}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aleksandrmorozov123/deep-learning-for-nlp?scriptVersionId=191046716\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-13T06:50:24.943155Z","iopub.execute_input":"2024-07-13T06:50:24.9436Z","iopub.status.idle":"2024-07-13T06:50:25.449162Z","shell.execute_reply.started":"2024-07-13T06:50:24.943561Z","shell.execute_reply":"2024-07-13T06:50:25.447865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checking statistics of the Corpus**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport pandas as pd\n\n# we get first 10 000 values for fast running\nratings = pd.read_csv ('/kaggle/input/massive-stock-news-analysis-db-for-nlpbacktests/raw_analyst_ratings.csv')[0:10000]\nratings.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:50:25.452176Z","iopub.execute_input":"2024-07-13T06:50:25.454326Z","iopub.status.idle":"2024-07-13T06:50:36.717727Z","shell.execute_reply.started":"2024-07-13T06:50:25.45427Z","shell.execute_reply":"2024-07-13T06:50:36.716075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# comparing the text of two selected ratings\nprint (repr(ratings.iloc[3399]['headline'][0:300]))\nprint (repr(ratings.iloc[5487]['headline'][0:300]))","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:50:36.719201Z","iopub.execute_input":"2024-07-13T06:50:36.719565Z","iopub.status.idle":"2024-07-13T06:50:36.72797Z","shell.execute_reply.started":"2024-07-13T06:50:36.719531Z","shell.execute_reply":"2024-07-13T06:50:36.726244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ignore spaces after the stop words\nimport re\nratings [\"paragraphs\"] = ratings [\"headline\"].map (lambda text: re.split ('[.?!]\\s*\\n', text))\nratings ['number_of_paragraphs'] = ratings [\"paragraphs\"].map (len)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:50:36.73185Z","iopub.execute_input":"2024-07-13T06:50:36.732287Z","iopub.status.idle":"2024-07-13T06:50:36.781331Z","shell.execute_reply.started":"2024-07-13T06:50:36.732252Z","shell.execute_reply":"2024-07-13T06:50:36.780137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preparations**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport sklearn\nimport spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom spacy.lang.de.stop_words import STOP_WORDS\n\ntfidf_text_vectorizer = TfidfVectorizer(stop_words=list(STOP_WORDS))\nvectors_text = tfidf_text_vectorizer.fit_transform (ratings ['headline'])\nvectors_text.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:50:36.782604Z","iopub.execute_input":"2024-07-13T06:50:36.782991Z","iopub.status.idle":"2024-07-13T06:50:43.230119Z","shell.execute_reply.started":"2024-07-13T06:50:36.782957Z","shell.execute_reply":"2024-07-13T06:50:43.228717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# flatten the paragraphs keeping the sentiment\nparagraph_df = pd.DataFrame ([{'headline': paragraph, 'publisher': publisher}\n                             for paragraphs, publisher in \\\n                             zip (ratings ['paragraphs'], ratings ['publisher'])\n                             for paragraph in paragraphs if paragraph])\ntfidf_para_vectorizer = TfidfVectorizer(stop_words=list(STOP_WORDS))\ntfidf_para_vectors = tfidf_para_vectorizer.fit_transform (paragraph_df ['headline'])\ntfidf_para_vectors.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:50:43.231683Z","iopub.execute_input":"2024-07-13T06:50:43.232328Z","iopub.status.idle":"2024-07-13T06:50:43.513694Z","shell.execute_reply.started":"2024-07-13T06:50:43.23229Z","shell.execute_reply":"2024-07-13T06:50:43.51232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Nonnegative matrix factorization** - $ V \\approx W \\cdot H $","metadata":{}},{"cell_type":"code","source":"# import required library\nfrom sklearn.decomposition import NMF\n\nnmf_text_model = NMF (n_components = 10, random_state = 42)\nW_text_matrix = nmf_text_model.fit_transform (vectors_text)\nH_text_matrix = nmf_text_model.components_\n\n# define a function for outputtin a summary\ndef display_topics (model, features, no_top_words=5):\n    for topic, word_vector in enumerate (nmf_text_model.components_):\n        total = word_vector.sum ()\n        largest = word_vector.argsort ()[::-1]  # invert sort order\n        print (\"\\ntopic %02d\" % topic)\n        for i in range (0, no_top_words):\n            print (\"  %s (%2.2f)\" % (features [largest [i]],\n                                    word_vector [largest[i]] * 100.0/total))\n            \n# calling the function\ndisplay_topics (nmf_text_model, tfidf_text_vectorizer.get_feature_names_out())","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:50:43.515329Z","iopub.execute_input":"2024-07-13T06:50:43.515778Z","iopub.status.idle":"2024-07-13T06:50:44.450984Z","shell.execute_reply.started":"2024-07-13T06:50:43.515737Z","shell.execute_reply":"2024-07-13T06:50:44.449747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# normalizing topics\nW_text_matrix.sum (axis=0)/W_text_matrix.sum()*100.0","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:50:44.452608Z","iopub.execute_input":"2024-07-13T06:50:44.453108Z","iopub.status.idle":"2024-07-13T06:50:44.462994Z","shell.execute_reply.started":"2024-07-13T06:50:44.453063Z","shell.execute_reply":"2024-07-13T06:50:44.461624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create a topic model for paragraphs using NMF**","metadata":{}},{"cell_type":"code","source":"nmf_para_model = NMF (n_components = 10, random_state = 42)\nW_para_matrix = nmf_para_model.fit_transform (tfidf_para_vectors)\nH_para_matrix = nmf_para_model.components_\n\ndisplay_topics (nmf_para_model, tfidf_para_vectorizer.get_feature_names_out ())","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:50:44.465052Z","iopub.execute_input":"2024-07-13T06:50:44.465575Z","iopub.status.idle":"2024-07-13T06:50:44.877136Z","shell.execute_reply.started":"2024-07-13T06:50:44.46553Z","shell.execute_reply":"2024-07-13T06:50:44.875659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Latent semantic analysis with singular value decomposition** - any $ m \\times n $ matrix V can be decomposed as follows\n$V = U \\cdot \\Sigma \\cdot V^* $","metadata":{}},{"cell_type":"code","source":"# import required module\nfrom sklearn.decomposition import TruncatedSVD\n\nsvd_para_model = TruncatedSVD (n_components = 10, random_state = 42)\nW_svd_para_matrix = svd_para_model.fit_transform (tfidf_para_vectors)\nH_svd_para_matrix = svd_para_model.components_\n\ndisplay_topics (svd_para_model, tfidf_para_vectorizer.get_feature_names_out ())","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:50:44.885658Z","iopub.execute_input":"2024-07-13T06:50:44.886566Z","iopub.status.idle":"2024-07-13T06:50:45.175488Z","shell.execute_reply.started":"2024-07-13T06:50:44.886503Z","shell.execute_reply":"2024-07-13T06:50:45.173758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Latent Dirichlet Allocation**","metadata":{}},{"cell_type":"code","source":"# import required modules\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\ncount_para_vectorizer = CountVectorizer (stop_words=list(STOP_WORDS))\ncount_para_vectors = count_para_vectorizer.fit_transform (paragraph_df ['headline'])\n\nlda_para_model = LatentDirichletAllocation (n_components = 10, random_state = 42)\nW_lda_para_matrix = lda_para_model.fit_transform (count_para_vectors)\nH_lda_para_matrix = lda_para_model.components_\n\ndisplay_topics (lda_para_model, tfidf_para_vectorizer.get_feature_names_out ())","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:50:45.177931Z","iopub.execute_input":"2024-07-13T06:50:45.179085Z","iopub.status.idle":"2024-07-13T06:51:15.531927Z","shell.execute_reply.started":"2024-07-13T06:50:45.179008Z","shell.execute_reply":"2024-07-13T06:51:15.530711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create Word Clouds to display and compare topic models**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\ndef wordcloud_topics (model, features, no_top_words = 40):\n    for topic, words in enumerate (model.components_):\n        size = {}\n        largest = words.argsort ()[::-1]  # invert sort order\n        for i in range (0, no_top_words):\n            size [features [largest [i]]] = abs (words [largest [i]])\n        wc = WordCloud (background_color = \"white\", max_words = 100,\n                       width = 960, height = 540)\n        wc.generate_from_frequencies (size)\n        plt.figure (figsize = (12, 12))\n        plt.imshow (wc, interpolation = 'bilinear')\n        plt.axis ('off')\n        \n# compare NMF and LDA model\nwordcloud_topics (nmf_para_model, tfidf_para_vectorizer.get_feature_names_out())\nwordcloud_topics (lda_para_model, count_para_vectorizer.get_feature_names_out ())","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:51:15.533445Z","iopub.execute_input":"2024-07-13T06:51:15.533887Z","iopub.status.idle":"2024-07-13T06:51:32.131998Z","shell.execute_reply.started":"2024-07-13T06:51:15.53384Z","shell.execute_reply":"2024-07-13T06:51:32.130624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Building a neural network using Pytorch**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport torch\nimport torch.nn as nn\n\nx = [[2, 5], [7, 9], [4, 8], [6, 9]]\ny = [[4], [9], [12], [17]]\n\nX = torch.tensor (x).float ()\nY = torch.tensor (y).float ()\n\ndevice = 'cuda' if torch.cuda.is_available () else 'cpu'\nX = X.to(device)\nY = Y.to(device)\n\nclass MyNeuralNet (nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.input_to_hidden_layer = nn.Linear (2, 8)\n        self.hidden_layer_activation = nn.ReLU()\n        self.hidden_to_output_layer = nn.Linear (8, 1)\n    def forward (self, x):\n        x = self.input_to_hidden_layer (x)\n        x = self.hidden_layer_activation (x)\n        x = self.hidden_to_output_layer (x)\n        return x\n    \nmynet = MyNeuralNet().to(device)\nloss_func = nn.MSELoss()\n\n_Y = mynet(X)\nloss_value = loss_func (_Y, Y)\nprint (loss_value)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:51:32.133797Z","iopub.execute_input":"2024-07-13T06:51:32.134302Z","iopub.status.idle":"2024-07-13T06:51:32.293212Z","shell.execute_reply.started":"2024-07-13T06:51:32.134256Z","shell.execute_reply":"2024-07-13T06:51:32.291939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.optim import SGD\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nopt = SGD (mynet.parameters(), lr = 0.001)\n\nloss_history = []\nfor _ in range(50):\n    opt.zero_grad()\n    loss_value = loss_func (mynet (X), Y)\n    loss_value.backward ()\n    opt.step ()\n    loss_history.append (loss_value.item())\n    \nplt.plot(loss_history)\nplt.title ('Loss variation over increasing epochs')\nplt.xlabel ('epochs')\nplt.ylabel ('loss value')","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:51:32.295132Z","iopub.execute_input":"2024-07-13T06:51:32.295599Z","iopub.status.idle":"2024-07-13T06:51:32.694723Z","shell.execute_reply.started":"2024-07-13T06:51:32.29556Z","shell.execute_reply":"2024-07-13T06:51:32.69332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Resnet block architecture**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nclass ResLayer (nn.Module):\n    def __init__ (self, ni, no, kernel_size, stride=1):\n        super (ResLayer, self).__init__()\n        padding = kernel_size - 2\n        self_conv = nn.Sequential (\n        nn.Conv2d (ni, no, kernel_size, stride,\n                  padding = padding),\n        nn.ReLU ())\n        \n    def forward (self, x):\n        return self.conv (x) + x","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:51:32.696806Z","iopub.execute_input":"2024-07-13T06:51:32.697286Z","iopub.status.idle":"2024-07-13T06:51:32.706599Z","shell.execute_reply.started":"2024-07-13T06:51:32.69725Z","shell.execute_reply":"2024-07-13T06:51:32.705267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nfrom torchvision import transforms,models,datasets\n!pip install torch_summary\nfrom torchsummary import summary\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmodel = models.vgg16(pretrained=True).to(device)\nsummary(model, torch.zeros(1,3,224,224))","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:51:32.70856Z","iopub.execute_input":"2024-07-13T06:51:32.709178Z","iopub.status.idle":"2024-07-13T06:51:55.058616Z","shell.execute_reply.started":"2024-07-13T06:51:32.709128Z","shell.execute_reply":"2024-07-13T06:51:55.057145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:51:55.060909Z","iopub.execute_input":"2024-07-13T06:51:55.061841Z","iopub.status.idle":"2024-07-13T06:51:55.072643Z","shell.execute_reply.started":"2024-07-13T06:51:55.061786Z","shell.execute_reply":"2024-07-13T06:51:55.071017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**RNN with TensorFlow**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport re\nimport os\n\nDATA_DIR = \"./data\"\nCHECKPOINT_DIR = os.path.join(DATA_DIR, \"checkpoints\")\n\ndef download_and_read (urls):\n    texts = []\n    for i, url in enumerate (urls):\n        p = tf.keras.utils.get_file (\"ex1-{:d}.txt\".format (i), url, cache_dir = \".\")\n        text = open (p, \"r\").read ()\n        # remove byte order mark\n        text = text.replace (\"\\ufeff\", \"\")\n        # remove new lines\n        text = text.replace ('\\n', ' ')\n        text = re.sub (r'\\s+', \" \", text)\n        # add it to the list\n        texts.extend (text)\n    return texts\n\ntexts = download_and_read ([\"http://www.gutenberg.org/cache/epub/28885/pg28885.txt\",\n\"https://www.gutenberg.org/files/12/12-0.txt\"])","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:51:55.075946Z","iopub.execute_input":"2024-07-13T06:51:55.076394Z","iopub.status.idle":"2024-07-13T06:52:11.598559Z","shell.execute_reply.started":"2024-07-13T06:51:55.07635Z","shell.execute_reply":"2024-07-13T06:52:11.597169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create the vocabulary\nvocab = sorted (set (texts))\nprint (\"vocab size: {:d}\".format (len(vocab)))\n\n# create mapping from vocab chars to ints\nchar2idx = {c:i for i, c in enumerate (vocab)}\nidx2char = {i:c for c, i in char2idx.items ()}\n\n# numericize the texts\ntexts_as_ints = np.array ([char2idx[c] for c in texts])\ndata = tf.data.Dataset.from_tensor_slices (texts_as_ints)\n\n# number of characters to show before asking for prediction sequences: [None, 100]\nseq_length = 100\nsequences = data.batch (seq_length + 1, drop_remainder = True)\n\ndef split_train_labels (sequence):\n    input_seq = sequence [0:-1]\n    output_seq = sequence [1:]\n    return input_seq, output_seq\n\nsequences = sequences.map (split_train_labels)\n\n# set up for training batches: [None, 64, 100]\nbatch_size = 64\nsteps_per_epoch = len (texts) // seq_length // batch_size\ndataset = sequences.shuffle (10000).batch (batch_size, drop_remainder = True)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:52:11.600076Z","iopub.execute_input":"2024-07-13T06:52:11.600904Z","iopub.status.idle":"2024-07-13T06:52:11.863825Z","shell.execute_reply.started":"2024-07-13T06:52:11.600734Z","shell.execute_reply":"2024-07-13T06:52:11.862249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CharGenModel (tf.keras.Model):\n    def __init__ (self, vocab_size, num_timesteps, embedding_dim, **kwargs):\n        super (CharGenModel, self).__init__(**kwargs)\n        self.embedding_layer = tf.keras.layers.Embedding (vocab_size, embedding_dim)\n        self.rnn_layer = tf.keras.layers.GRU(\n        num_timesteps,\n        recurrent_initializer = 'glorot_uniform',\n        recurrent_activation = 'sigmoid',\n        stateful = True,\n        return_sequences = True)\n        self.dense_layer = tf.keras.layers.Dense (vocab_size)\n        \n    def call (self, x):\n        x = self.embedding_layer (x)\n        x = self.rnn_layer (x)\n        x = self.dense_layer (x)\n        return x\n    \nvocab_size = len (vocab)\nembedding_dim = 256\n\nmodel = CharGenModel (vocab_size, seq_length, embedding_dim)\nmodel.build (input_shape = (batch_size, seq_length))","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:52:11.865586Z","iopub.execute_input":"2024-07-13T06:52:11.866119Z","iopub.status.idle":"2024-07-13T06:52:12.290477Z","shell.execute_reply.started":"2024-07-13T06:52:11.866068Z","shell.execute_reply":"2024-07-13T06:52:12.288957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loss (labels, predictions):\n    return tf.losses.sparse_categorical_crossentropy (\n    labels, predictions, from_logits = True)\n\nmodel.compile (optimizer = tf.optimizers.Adam (), loss = loss)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:52:12.292352Z","iopub.execute_input":"2024-07-13T06:52:12.29294Z","iopub.status.idle":"2024-07-13T06:52:12.320201Z","shell.execute_reply.started":"2024-07-13T06:52:12.29289Z","shell.execute_reply":"2024-07-13T06:52:12.318933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_text(model, prefix_string, char2idx, idx2char,\n                  num_chars_to_generate=1000, temperature=1.0):\n    input = [char2idx[s] for s in prefix_string]\n    input = tf.expand_dims(input, 0)\n    text_generated = []\n    model.reset_states()\n    for i in range(num_chars_to_generate):\n        preds = model(input)\n        preds = tf.squeeze(preds, 0) / temperature\n        # predict char returned by model\n        pred_id = tf.random.categorical(\n            preds, num_samples=1)[-1, 0].numpy()\n        text_generated.append(idx2char[pred_id])\n        # pass the prediction as the next input to the model\n        input = tf.expand_dims([pred_id], 0)\n    return prefix_string + \"\".join(text_generated)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:52:12.321718Z","iopub.execute_input":"2024-07-13T06:52:12.322229Z","iopub.status.idle":"2024-07-13T06:52:12.331235Z","shell.execute_reply.started":"2024-07-13T06:52:12.322184Z","shell.execute_reply":"2024-07-13T06:52:12.329972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 50\nfor i in range(num_epochs // 10):\n    model.fit(\n        dataset.repeat(), epochs=10,\n        steps_per_epoch=steps_per_epoch\n        # callbacks=[checkpoint_callback, tensorboard_callback]\n)\n    checkpoint_file = os.path.join(CHECKPOINT_DIR, \"model_epoch_{:d}\".format(i+1))\n    model.save_weights(checkpoint_file)\n    # create generative model using the trained model so far\n    gen_model = CharGenModel(vocab_size, seq_length, embedding_dim)\n    gen_model.load_weights(checkpoint_file)\n    gen_model.build(input_shape=(1, seq_length))\n    print(\"after epoch: {:d}\".format(i+1)*10)\n    print(generate_text(gen_model, \"Alice \", char2idx, idx2char))\n    print(\"---\")","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:52:12.332642Z","iopub.execute_input":"2024-07-13T06:52:12.333078Z","iopub.status.idle":"2024-07-13T06:59:21.428766Z","shell.execute_reply.started":"2024-07-13T06:52:12.333044Z","shell.execute_reply":"2024-07-13T06:59:21.427609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download(\"treebank\")\n\ndef download_and_read(dataset_dir, num_pairs=None):\n    sent_filename = os.path.join(dataset_dir, \"treebank-sents.txt\")\n    poss_filename = os.path.join(dataset_dir, \"treebank-poss.txt\")\n    if not(os.path.exists(sent_filename) and os.path.exists(poss_filename)):\n        import nltk\n        if not os.path.exists(dataset_dir):\n            os.makedirs(dataset_dir)\n        fsents = open(sent_filename, \"w\")\n        fposs = open(poss_filename, \"w\")\n        sentences = nltk.corpus.treebank.tagged_sents()\n        for sent in sentences:\n            fsents.write(\" \".join([w for w, p in sent]) + \"\\n\")\n            fposs.write(\" \".join([p for w, p in sent]) + \"\\n\")\n        fsents.close()\n        fposs.close()\n    sents, poss = [], []\n    with open(sent_filename, \"r\") as fsent:\n        for idx, line in enumerate(fsent):\n            sents.append(line.strip())\n            if num_pairs is not None and idx >= num_pairs:\n                break\n    with open(poss_filename, \"r\") as fposs:\n        for idx, line in enumerate(fposs):\n            poss.append(line.strip())\n            if num_pairs is not None and idx >= num_pairs:\n                break\n    return sents, poss\nsents, poss = download_and_read(\"./datasets\")\nassert(len(sents) == len(poss))\nprint(\"# of records: {:d}\".format(len(sents)))","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:59:21.430456Z","iopub.execute_input":"2024-07-13T06:59:21.430843Z","iopub.status.idle":"2024-07-13T06:59:24.39846Z","shell.execute_reply.started":"2024-07-13T06:59:21.430809Z","shell.execute_reply":"2024-07-13T06:59:24.396963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_and_build_vocab(texts, vocab_size=None, lower=True):\n    if vocab_size is None:\n        tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=lower)\n    else:\n        tokenizer = tf.keras.preprocessing.text.Tokenizer(\n            num_words=vocab_size+1, oov_token=\"UNK\", lower=lower)\n    tokenizer.fit_on_texts(texts)\n    if vocab_size is not None:\n        # additional workaround, see issue 8092\n        # https://github.com/keras-team/keras/issues/8092\n        tokenizer.word_index = {e:i for e, i in \n                                tokenizer.word_index.items() if \n                                i <= vocab_size+1 }\n    word2idx = tokenizer.word_index\n    idx2word = {v:k for k, v in word2idx.items()}\n    return word2idx, idx2word, tokenizer\nword2idx_s, idx2word_s, tokenizer_s = tokenize_and_build_vocab(sents, vocab_size=9000)\nword2idx_t, idx2word_t, tokenizer_t = tokenize_and_build_vocab(poss, vocab_size=38, lower=False)\nsource_vocab_size = len(word2idx_s)\ntarget_vocab_size = len(word2idx_t)\nprint(\"vocab sizes (source): {:d}, (target): {:d}\".format(\n    source_vocab_size, target_vocab_size))","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:59:24.400274Z","iopub.execute_input":"2024-07-13T06:59:24.40106Z","iopub.status.idle":"2024-07-13T06:59:24.69015Z","shell.execute_reply.started":"2024-07-13T06:59:24.401009Z","shell.execute_reply":"2024-07-13T06:59:24.688808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence_lengths = np.array([len(s.split()) for s in sents])\nprint([(p, np.percentile(sequence_lengths, p))\n       for p in [75, 80, 90, 95, 99, 100]])\n[(75, 33.0), (80, 35.0), (90, 41.0), (95, 47.0), (99, 58.0), (100, 271.0)]","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:59:24.691654Z","iopub.execute_input":"2024-07-13T06:59:24.692075Z","iopub.status.idle":"2024-07-13T06:59:24.720732Z","shell.execute_reply.started":"2024-07-13T06:59:24.69204Z","shell.execute_reply":"2024-07-13T06:59:24.719277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_seqlen = 271\n\n# convert sentences to sequence of integers\nsents_as_ints = tokenizer_s.texts_to_sequences(sents)\nsents_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n    sents_as_ints, maxlen=max_seqlen, padding=\"post\")\n\n# convert POS tags to sequence of (categorical) integers\nposs_as_ints = tokenizer_t.texts_to_sequences(poss)\nposs_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n    poss_as_ints, maxlen=max_seqlen, padding=\"post\")\nposs_as_catints = []\n\nfor p in poss_as_ints:\n    poss_as_catints.append(tf.keras.utils.to_categorical(p,num_classes=target_vocab_size+1, dtype=\"int32\"))\nposs_as_catints = tf.keras.preprocessing.sequence.pad_sequences(\n    poss_as_catints, maxlen=max_seqlen)\n\ndataset = tf.data.Dataset.from_tensor_slices(\n    (sents_as_ints, poss_as_catints))\nidx2word_s[0], idx2word_t[0] = \"PAD\", \"PAD\"\n\n# split into training, validation, and test datasets\ndataset = dataset.shuffle(10000)\ntest_size = len(sents) // 3\nval_size = (len(sents) - test_size) // 10\n\ntest_dataset = dataset.take(test_size)\nval_dataset = dataset.skip(test_size).take(val_size)\ntrain_dataset = dataset.skip(test_size + val_size)\n\n# create batches\nbatch_size = 128\ntrain_dataset = train_dataset.batch(batch_size)\nval_dataset = val_dataset.batch(batch_size)\ntest_dataset = test_dataset.batch(batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:59:24.723424Z","iopub.execute_input":"2024-07-13T06:59:24.723943Z","iopub.status.idle":"2024-07-13T06:59:25.648064Z","shell.execute_reply.started":"2024-07-13T06:59:24.723899Z","shell.execute_reply":"2024-07-13T06:59:25.646821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def masked_accuracy():\n    def masked_accuracy_fn(ytrue, ypred):\n        ytrue = tf.keras.backend.argmax(ytrue, axis=-1)\n        ypred = tf.keras.backend.argmax(ypred, axis=-1)\n        mask = tf.keras.backend.cast(\n            tf.keras.backend.not_equal(ypred, 0), tf.int32)\n        matches = tf.keras.backend.cast(\n            tf.keras.backend.equal(ytrue, ypred), tf.int32) * mask\n        numer = tf.keras.backend.sum(matches)\n        denom = tf.keras.backend.maximum(tf.keras.backend.sum(mask), 1)\n        accuracy = numer / denom\n        return accuracy\n    return masked_accuracy_fn","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:59:25.65515Z","iopub.execute_input":"2024-07-13T06:59:25.655569Z","iopub.status.idle":"2024-07-13T06:59:25.664741Z","shell.execute_reply.started":"2024-07-13T06:59:25.655535Z","shell.execute_reply":"2024-07-13T06:59:25.66319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class POSTaggingModel(tf.keras.Model):\n    def __init__(self, source_vocab_size, target_vocab_size,\n                 embedding_dim, max_seqlen, rnn_output_dim, **kwargs):\n        super(POSTaggingModel, self).__init__(**kwargs)\n        self.embed = tf.keras.layers.Embedding(\n            source_vocab_size, embedding_dim, input_length=max_seqlen)\n        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n        self.rnn = tf.keras.layers.Bidirectional(\n            tf.keras.layers.GRU(rnn_output_dim, return_sequences=True))\n        self.dense = tf.keras.layers.TimeDistributed(\n            tf.keras.layers.Dense(target_vocab_size))\n        self.activation = tf.keras.layers.Activation(\"softmax\")\n    def call(self, x):\n        x = self.embed(x)\n        x = self.dropout(x)\n        x = self.rnn(x)\n        x = self.dense(x)\n        x = self.activation(x)\n        return x\nembedding_dim = 128\nrnn_output_dim = 256\nmodel = POSTaggingModel(source_vocab_size, target_vocab_size,\n                        embedding_dim, max_seqlen, rnn_output_dim)\nmodel.build(input_shape=(batch_size, max_seqlen))\nmodel.summary()\nmodel.compile(\n    loss=\"categorical_crossentropy\",\n    optimizer=\"adam\",\n    metrics=[\"accuracy\", masked_accuracy()])","metadata":{"execution":{"iopub.status.busy":"2024-07-13T06:59:25.666325Z","iopub.execute_input":"2024-07-13T06:59:25.666689Z","iopub.status.idle":"2024-07-13T06:59:26.406976Z","shell.execute_reply.started":"2024-07-13T06:59:25.666654Z","shell.execute_reply":"2024-07-13T06:59:26.404735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Transformers**","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow_datasets\n!pip install -U 'tensorflow-text==2.8.*'\nimport logging\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow_text\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\nlogging.getLogger('tensorflow').setLevel(logging.ERROR) # suppress warnings","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True, as_supervised=True)\ntrain_examples, val_examples = examples['train'], examples['validation']\n\nmodel_name = 'ted_hrlr_translate_pt_en_converter'\ntf.keras.utils.get_file(f'{model_name}.zip',f'https://storage.googleapis.com/download.tensorflow.org/models/\n                        {model_name}.zip',cache_dir='.', cache_subdir='', extract=True)\ntokenizers = tf.saved_model.load(model_name)\n                        \nfor pt_examples, en_examples in train_examples.batch(3).take(1):\n                        print('> Examples in Portuguese:')\nfor en in en_examples.numpy():\n                        print(en.decode('utf-8'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded = tokenizers.en.tokenize(en_examples)\nfor row in encoded.to_list():\n    print(row)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"round_trip = tokenizers.en.detokenize(encoded)\nfor line in round_trip.numpy():\n    print(line.decode('utf-8'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_TOKENS=128\ndef filter_max_tokens(pt, en):\n    num_tokens = tf.maximum(tf.shape(pt)[1],tf.shape(en)[1])\n    return num_tokens < MAX_TOKENS\n\ndef tokenize_pairs(pt, en):\n    pt = tokenizers.pt.tokenize(pt)\n    # Convert from ragged to dense, padding with zeros.\n    pt = pt.to_tensor()\n    en = tokenizers.en.tokenize(en)\n    # Convert from ragged to dense, padding with zeros.\n    en = en.to_tensor()\n    return pt, en\n\nBUFFER_SIZE = 20000\nBATCH_SIZE = 64\ndef make_batches(ds):\n    return (\n        ds.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n        .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n        .filter(filter_max_tokens).prefetch(tf.data.AUTOTUNE))\n\ntrain_batches = make_batches(train_examples)\nval_batches = make_batches(val_examples)\n\ndef get_angles(pos, i, d_model):\n    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n    return pos * angle_rates\ndef positional_encoding(position, d_model):\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                            np.arange(d_model)[np.newaxis, :],d_model)\n    \n# apply sin to even indices in the array; 2i\nangle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n\n# apply cos to odd indices in the array; 2i+1\nangle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\npos_encoding = angle_rads[np.newaxis, ...]\n\nreturn tf.cast(pos_encoding, dtype=tf.float32)\n\ndef create_padding_mask(seq):\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n    # add extra dimensions to add the padding\n    # to the attention logits.\n    return seq[:, tf.newaxis, tf.newaxis, :] # (batch_size, 1, 1, seq_len)\n\ndef create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask # (seq_len, seq_len)\n\ndef scaled_dot_product_attention(q, k, v, mask):\n    \"\"\"Calculate the attention weights.\n    q, k, v must have matching leading dimensions.\n    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n    The mask has different shapes depending on its type(padding or look ahead)\n    but it must be broadcastable for addition.\n    Args:\n    q: query shape == (..., seq_len_q, depth)\n    k: key shape == (..., seq_len_k, depth)\n    v: value shape == (..., seq_len_v, depth_v)\n    mask: Float tensor with shape broadcastable\n    to (..., seq_len_q, seq_len_k). Defaults to None.\n    Returns:\n    output, attention_weights\n    \"\"\"\n    matmul_qk = tf.matmul(q, k, transpose_b=True) # (..., seq_len_q, seq_len_k)\n    # scale matmul_qk\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n    # add the mask to the scaled tensor.\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)\n    # softmax is normalized on the last axis (seq_len_k) so that the scores\n    # add up to 1.\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) #(..., seq_len_q, seq_len_k)\n    output = tf.matmul(attention_weights, v) # (..., seq_len_q, depth_v)\n    return output, attention_weights\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self,*, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        assert d_model % self.num_heads == 0\n        self.depth = d_model // self.num_heads\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n        self.dense = tf.keras.layers.Dense(d_model)\n    \n    def split_heads(self, x, batch_size):\n        \"\"\"Split the last dimension into (num_heads, depth).\n        Transpose the result such that the shape is (batch_size, num_heads,seq_len, depth)\n        \"\"\"\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n        q = self.wq(q) # (batch_size, seq_len, d_model)\n        k = self.wk(k) # (batch_size, seq_len, d_model)\n        v = self.wv(v) # (batch_size, seq_len, d_model)\n        q = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size) # (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size) # (batch_size, num_heads, seq_len_v, depth) \n        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n        # (batch_size, seq_len_q, num_heads, depth)\n        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model)) # (batch_size, seq_len_q, d_model)\n        output = self.dense(concat_attention) # (batch_size, seq_len_q, d_model)\n        return output, attention_weights","metadata":{},"execution_count":null,"outputs":[]}]}