{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":796646,"sourceType":"datasetVersion","datasetId":19136},{"sourceId":928056,"sourceType":"datasetVersion","datasetId":500989},{"sourceId":1304644,"sourceType":"datasetVersion","datasetId":754810}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aleksandrmorozov123/deep-learning-for-nlp?scriptVersionId=196243873\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-06T17:54:45.337596Z","iopub.execute_input":"2024-08-06T17:54:45.338548Z","iopub.status.idle":"2024-08-06T17:54:45.757451Z","shell.execute_reply.started":"2024-08-06T17:54:45.338505Z","shell.execute_reply":"2024-08-06T17:54:45.756155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checking statistics of the Corpus**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport pandas as pd\n\n# we get first 10 000 values for fast running\nratings = pd.read_csv ('/kaggle/input/massive-stock-news-analysis-db-for-nlpbacktests/raw_analyst_ratings.csv')[0:10000]\nratings.info()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:54:45.75942Z","iopub.execute_input":"2024-08-06T17:54:45.759898Z","iopub.status.idle":"2024-08-06T17:54:55.982439Z","shell.execute_reply.started":"2024-08-06T17:54:45.759866Z","shell.execute_reply":"2024-08-06T17:54:55.981179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# comparing the text of two selected ratings\nprint (repr(ratings.iloc[3399]['headline'][0:300]))\nprint (repr(ratings.iloc[5487]['headline'][0:300]))","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:54:55.983947Z","iopub.execute_input":"2024-08-06T17:54:55.984348Z","iopub.status.idle":"2024-08-06T17:54:55.990466Z","shell.execute_reply.started":"2024-08-06T17:54:55.984305Z","shell.execute_reply":"2024-08-06T17:54:55.989395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ignore spaces after the stop words\nimport re\nratings [\"paragraphs\"] = ratings [\"headline\"].map (lambda text: re.split ('[.?!]\\s*\\n', text))\nratings ['number_of_paragraphs'] = ratings [\"paragraphs\"].map (len)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:54:55.993596Z","iopub.execute_input":"2024-08-06T17:54:55.994526Z","iopub.status.idle":"2024-08-06T17:54:56.024754Z","shell.execute_reply.started":"2024-08-06T17:54:55.994491Z","shell.execute_reply":"2024-08-06T17:54:56.023641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preparations**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport sklearn\nimport spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom spacy.lang.de.stop_words import STOP_WORDS\n\ntfidf_text_vectorizer = TfidfVectorizer(stop_words=list(STOP_WORDS))\nvectors_text = tfidf_text_vectorizer.fit_transform (ratings ['headline'])\nvectors_text.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:54:56.026376Z","iopub.execute_input":"2024-08-06T17:54:56.026832Z","iopub.status.idle":"2024-08-06T17:55:01.230181Z","shell.execute_reply.started":"2024-08-06T17:54:56.026799Z","shell.execute_reply":"2024-08-06T17:55:01.229162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# flatten the paragraphs keeping the sentiment\nparagraph_df = pd.DataFrame ([{'headline': paragraph, 'publisher': publisher}\n                             for paragraphs, publisher in \\\n                             zip (ratings ['paragraphs'], ratings ['publisher'])\n                             for paragraph in paragraphs if paragraph])\ntfidf_para_vectorizer = TfidfVectorizer(stop_words=list(STOP_WORDS))\ntfidf_para_vectors = tfidf_para_vectorizer.fit_transform (paragraph_df ['headline'])\ntfidf_para_vectors.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:55:01.231276Z","iopub.execute_input":"2024-08-06T17:55:01.231783Z","iopub.status.idle":"2024-08-06T17:55:01.430328Z","shell.execute_reply.started":"2024-08-06T17:55:01.231745Z","shell.execute_reply":"2024-08-06T17:55:01.429213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Nonnegative matrix factorization** - $ V \\approx W \\cdot H $","metadata":{}},{"cell_type":"code","source":"# import required library\nfrom sklearn.decomposition import NMF\n\nnmf_text_model = NMF (n_components = 10, random_state = 42)\nW_text_matrix = nmf_text_model.fit_transform (vectors_text)\nH_text_matrix = nmf_text_model.components_\n\n# define a function for outputtin a summary\ndef display_topics (model, features, no_top_words=5):\n    for topic, word_vector in enumerate (nmf_text_model.components_):\n        total = word_vector.sum ()\n        largest = word_vector.argsort ()[::-1]  # invert sort order\n        print (\"\\ntopic %02d\" % topic)\n        for i in range (0, no_top_words):\n            print (\"  %s (%2.2f)\" % (features [largest [i]],\n                                    word_vector [largest[i]] * 100.0/total))\n            \n# calling the function\ndisplay_topics (nmf_text_model, tfidf_text_vectorizer.get_feature_names_out())","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:55:01.431919Z","iopub.execute_input":"2024-08-06T17:55:01.432246Z","iopub.status.idle":"2024-08-06T17:55:02.248101Z","shell.execute_reply.started":"2024-08-06T17:55:01.432219Z","shell.execute_reply":"2024-08-06T17:55:02.246942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# normalizing topics\nW_text_matrix.sum (axis=0)/W_text_matrix.sum()*100.0","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:55:02.249397Z","iopub.execute_input":"2024-08-06T17:55:02.249725Z","iopub.status.idle":"2024-08-06T17:55:02.257246Z","shell.execute_reply.started":"2024-08-06T17:55:02.249673Z","shell.execute_reply":"2024-08-06T17:55:02.256159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create a topic model for paragraphs using NMF**","metadata":{}},{"cell_type":"code","source":"nmf_para_model = NMF (n_components = 10, random_state = 42)\nW_para_matrix = nmf_para_model.fit_transform (tfidf_para_vectors)\nH_para_matrix = nmf_para_model.components_\n\ndisplay_topics (nmf_para_model, tfidf_para_vectorizer.get_feature_names_out ())","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:55:02.258597Z","iopub.execute_input":"2024-08-06T17:55:02.258954Z","iopub.status.idle":"2024-08-06T17:55:02.637606Z","shell.execute_reply.started":"2024-08-06T17:55:02.258925Z","shell.execute_reply":"2024-08-06T17:55:02.636337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Latent semantic analysis with singular value decomposition** - any $ m \\times n $ matrix V can be decomposed as follows\n$V = U \\cdot \\Sigma \\cdot V^* $","metadata":{}},{"cell_type":"code","source":"# import required module\nfrom sklearn.decomposition import TruncatedSVD\n\nsvd_para_model = TruncatedSVD (n_components = 10, random_state = 42)\nW_svd_para_matrix = svd_para_model.fit_transform (tfidf_para_vectors)\nH_svd_para_matrix = svd_para_model.components_\n\ndisplay_topics (svd_para_model, tfidf_para_vectorizer.get_feature_names_out ())","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:55:02.643313Z","iopub.execute_input":"2024-08-06T17:55:02.644058Z","iopub.status.idle":"2024-08-06T17:55:02.784222Z","shell.execute_reply.started":"2024-08-06T17:55:02.644006Z","shell.execute_reply":"2024-08-06T17:55:02.783098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Latent Dirichlet Allocation**","metadata":{}},{"cell_type":"code","source":"# import required modules\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\ncount_para_vectorizer = CountVectorizer (stop_words=list(STOP_WORDS))\ncount_para_vectors = count_para_vectorizer.fit_transform (paragraph_df ['headline'])\n\nlda_para_model = LatentDirichletAllocation (n_components = 10, random_state = 42)\nW_lda_para_matrix = lda_para_model.fit_transform (count_para_vectors)\nH_lda_para_matrix = lda_para_model.components_\n\ndisplay_topics (lda_para_model, tfidf_para_vectorizer.get_feature_names_out ())","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:55:02.790682Z","iopub.execute_input":"2024-08-06T17:55:02.791427Z","iopub.status.idle":"2024-08-06T17:55:19.643643Z","shell.execute_reply.started":"2024-08-06T17:55:02.79138Z","shell.execute_reply":"2024-08-06T17:55:19.642557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create Word Clouds to display and compare topic models**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\ndef wordcloud_topics (model, features, no_top_words = 40):\n    for topic, words in enumerate (model.components_):\n        size = {}\n        largest = words.argsort ()[::-1]  # invert sort order\n        for i in range (0, no_top_words):\n            size [features [largest [i]]] = abs (words [largest [i]])\n        wc = WordCloud (background_color = \"white\", max_words = 100,\n                       width = 960, height = 540)\n        wc.generate_from_frequencies (size)\n        plt.figure (figsize = (12, 12))\n        plt.imshow (wc, interpolation = 'bilinear')\n        plt.axis ('off')\n        \n# compare NMF and LDA model\nwordcloud_topics (nmf_para_model, tfidf_para_vectorizer.get_feature_names_out())\nwordcloud_topics (lda_para_model, count_para_vectorizer.get_feature_names_out ())","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:55:19.645075Z","iopub.execute_input":"2024-08-06T17:55:19.64542Z","iopub.status.idle":"2024-08-06T17:55:35.580024Z","shell.execute_reply.started":"2024-08-06T17:55:19.645391Z","shell.execute_reply":"2024-08-06T17:55:35.578887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Building a neural network using Pytorch**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport torch\nimport torch.nn as nn\n\nx = [[2, 5], [7, 9], [4, 8], [6, 9]]\ny = [[4], [9], [12], [17]]\n\nX = torch.tensor (x).float ()\nY = torch.tensor (y).float ()\n\ndevice = 'cuda' if torch.cuda.is_available () else 'cpu'\nX = X.to(device)\nY = Y.to(device)\n\nclass MyNeuralNet (nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.input_to_hidden_layer = nn.Linear (2, 8)\n        self.hidden_layer_activation = nn.ReLU()\n        self.hidden_to_output_layer = nn.Linear (8, 1)\n    def forward (self, x):\n        x = self.input_to_hidden_layer (x)\n        x = self.hidden_layer_activation (x)\n        x = self.hidden_to_output_layer (x)\n        return x\n    \nmynet = MyNeuralNet().to(device)\nloss_func = nn.MSELoss()\n\n_Y = mynet(X)\nloss_value = loss_func (_Y, Y)\nprint (loss_value)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:55:35.58129Z","iopub.execute_input":"2024-08-06T17:55:35.58161Z","iopub.status.idle":"2024-08-06T17:55:35.71114Z","shell.execute_reply.started":"2024-08-06T17:55:35.581581Z","shell.execute_reply":"2024-08-06T17:55:35.710037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.optim import SGD\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nopt = SGD (mynet.parameters(), lr = 0.001)\n\nloss_history = []\nfor _ in range(50):\n    opt.zero_grad()\n    loss_value = loss_func (mynet (X), Y)\n    loss_value.backward ()\n    opt.step ()\n    loss_history.append (loss_value.item())\n    \nplt.plot(loss_history)\nplt.title ('Loss variation over increasing epochs')\nplt.xlabel ('epochs')\nplt.ylabel ('loss value')","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:55:35.712306Z","iopub.execute_input":"2024-08-06T17:55:35.712723Z","iopub.status.idle":"2024-08-06T17:55:36.042352Z","shell.execute_reply.started":"2024-08-06T17:55:35.712666Z","shell.execute_reply":"2024-08-06T17:55:36.041209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Resnet block architecture**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nclass ResLayer (nn.Module):\n    def __init__ (self, ni, no, kernel_size, stride=1):\n        super (ResLayer, self).__init__()\n        padding = kernel_size - 2\n        self_conv = nn.Sequential (\n        nn.Conv2d (ni, no, kernel_size, stride,\n                  padding = padding),\n        nn.ReLU ())\n        \n    def forward (self, x):\n        return self.conv (x) + x","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:55:36.043634Z","iopub.execute_input":"2024-08-06T17:55:36.043988Z","iopub.status.idle":"2024-08-06T17:55:36.050603Z","shell.execute_reply.started":"2024-08-06T17:55:36.043958Z","shell.execute_reply":"2024-08-06T17:55:36.049411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nfrom torchvision import transforms,models,datasets\n!pip install torch_summary\nfrom torchsummary import summary\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmodel = models.vgg16(pretrained=True).to(device)\nsummary(model, torch.zeros(1,3,224,224))","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:55:36.051919Z","iopub.execute_input":"2024-08-06T17:55:36.052282Z","iopub.status.idle":"2024-08-06T17:55:53.433763Z","shell.execute_reply.started":"2024-08-06T17:55:36.05225Z","shell.execute_reply":"2024-08-06T17:55:53.43257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:55:53.43538Z","iopub.execute_input":"2024-08-06T17:55:53.435762Z","iopub.status.idle":"2024-08-06T17:55:53.443632Z","shell.execute_reply.started":"2024-08-06T17:55:53.435725Z","shell.execute_reply":"2024-08-06T17:55:53.442545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**RNN with TensorFlow**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport re\nimport os\n\nDATA_DIR = \"./data\"\nCHECKPOINT_DIR = os.path.join(DATA_DIR, \"checkpoints\")\n\ndef download_and_read (urls):\n    texts = []\n    for i, url in enumerate (urls):\n        p = tf.keras.utils.get_file (\"ex1-{:d}.txt\".format (i), url, cache_dir = \".\")\n        text = open (p, \"r\").read ()\n        # remove byte order mark\n        text = text.replace (\"\\ufeff\", \"\")\n        # remove new lines\n        text = text.replace ('\\n', ' ')\n        text = re.sub (r'\\s+', \" \", text)\n        # add it to the list\n        texts.extend (text)\n    return texts\n\ntexts = download_and_read ([\"http://www.gutenberg.org/cache/epub/28885/pg28885.txt\",\n\"https://www.gutenberg.org/files/12/12-0.txt\"])","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:55:53.445012Z","iopub.execute_input":"2024-08-06T17:55:53.445331Z","iopub.status.idle":"2024-08-06T17:56:08.230151Z","shell.execute_reply.started":"2024-08-06T17:55:53.445304Z","shell.execute_reply":"2024-08-06T17:56:08.229063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create the vocabulary\nvocab = sorted (set (texts))\nprint (\"vocab size: {:d}\".format (len(vocab)))\n\n# create mapping from vocab chars to ints\nchar2idx = {c:i for i, c in enumerate (vocab)}\nidx2char = {i:c for c, i in char2idx.items ()}\n\n# numericize the texts\ntexts_as_ints = np.array ([char2idx[c] for c in texts])\ndata = tf.data.Dataset.from_tensor_slices (texts_as_ints)\n\n# number of characters to show before asking for prediction sequences: [None, 100]\nseq_length = 100\nsequences = data.batch (seq_length + 1, drop_remainder = True)\n\ndef split_train_labels (sequence):\n    input_seq = sequence [0:-1]\n    output_seq = sequence [1:]\n    return input_seq, output_seq\n\nsequences = sequences.map (split_train_labels)\n\n# set up for training batches: [None, 64, 100]\nbatch_size = 64\nsteps_per_epoch = len (texts) // seq_length // batch_size\ndataset = sequences.shuffle (10000).batch (batch_size, drop_remainder = True)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:56:08.231771Z","iopub.execute_input":"2024-08-06T17:56:08.232616Z","iopub.status.idle":"2024-08-06T17:56:08.449016Z","shell.execute_reply.started":"2024-08-06T17:56:08.232571Z","shell.execute_reply":"2024-08-06T17:56:08.447914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CharGenModel (tf.keras.Model):\n    def __init__ (self, vocab_size, num_timesteps, embedding_dim, **kwargs):\n        super (CharGenModel, self).__init__(**kwargs)\n        self.embedding_layer = tf.keras.layers.Embedding (vocab_size, embedding_dim)\n        self.rnn_layer = tf.keras.layers.GRU(\n        num_timesteps,\n        recurrent_initializer = 'glorot_uniform',\n        recurrent_activation = 'sigmoid',\n        stateful = True,\n        return_sequences = True)\n        self.dense_layer = tf.keras.layers.Dense (vocab_size)\n        \n    def call (self, x):\n        x = self.embedding_layer (x)\n        x = self.rnn_layer (x)\n        x = self.dense_layer (x)\n        return x\n    \nvocab_size = len (vocab)\nembedding_dim = 256\n\nmodel = CharGenModel (vocab_size, seq_length, embedding_dim)\nmodel.build (input_shape = (batch_size, seq_length))","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:56:08.45035Z","iopub.execute_input":"2024-08-06T17:56:08.450663Z","iopub.status.idle":"2024-08-06T17:56:08.788305Z","shell.execute_reply.started":"2024-08-06T17:56:08.450636Z","shell.execute_reply":"2024-08-06T17:56:08.787202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loss (labels, predictions):\n    return tf.losses.sparse_categorical_crossentropy (\n    labels, predictions, from_logits = True)\n\nmodel.compile (optimizer = tf.optimizers.Adam (), loss = loss)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:56:08.789817Z","iopub.execute_input":"2024-08-06T17:56:08.79026Z","iopub.status.idle":"2024-08-06T17:56:08.812575Z","shell.execute_reply.started":"2024-08-06T17:56:08.790222Z","shell.execute_reply":"2024-08-06T17:56:08.811648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_text(model, prefix_string, char2idx, idx2char,\n                  num_chars_to_generate=1000, temperature=1.0):\n    input = [char2idx[s] for s in prefix_string]\n    input = tf.expand_dims(input, 0)\n    text_generated = []\n    model.reset_states()\n    for i in range(num_chars_to_generate):\n        preds = model(input)\n        preds = tf.squeeze(preds, 0) / temperature\n        # predict char returned by model\n        pred_id = tf.random.categorical(\n            preds, num_samples=1)[-1, 0].numpy()\n        text_generated.append(idx2char[pred_id])\n        # pass the prediction as the next input to the model\n        input = tf.expand_dims([pred_id], 0)\n    return prefix_string + \"\".join(text_generated)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:56:08.813998Z","iopub.execute_input":"2024-08-06T17:56:08.814824Z","iopub.status.idle":"2024-08-06T17:56:08.822763Z","shell.execute_reply.started":"2024-08-06T17:56:08.814782Z","shell.execute_reply":"2024-08-06T17:56:08.821552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 50\nfor i in range(num_epochs // 10):\n    model.fit(\n        dataset.repeat(), epochs=10,\n        steps_per_epoch=steps_per_epoch\n        # callbacks=[checkpoint_callback, tensorboard_callback]\n)\n    checkpoint_file = os.path.join(CHECKPOINT_DIR, \"model_epoch_{:d}\".format(i+1))\n    model.save_weights(checkpoint_file)\n    # create generative model using the trained model so far\n    gen_model = CharGenModel(vocab_size, seq_length, embedding_dim)\n    gen_model.load_weights(checkpoint_file)\n    gen_model.build(input_shape=(1, seq_length))\n    print(\"after epoch: {:d}\".format(i+1)*10)\n    print(generate_text(gen_model, \"Alice \", char2idx, idx2char))\n    print(\"---\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:56:08.824289Z","iopub.execute_input":"2024-08-06T17:56:08.824643Z","iopub.status.idle":"2024-08-06T18:02:12.458455Z","shell.execute_reply.started":"2024-08-06T17:56:08.824609Z","shell.execute_reply":"2024-08-06T18:02:12.457363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download(\"treebank\")\n\ndef download_and_read(dataset_dir, num_pairs=None):\n    sent_filename = os.path.join(dataset_dir, \"treebank-sents.txt\")\n    poss_filename = os.path.join(dataset_dir, \"treebank-poss.txt\")\n    if not(os.path.exists(sent_filename) and os.path.exists(poss_filename)):\n        import nltk\n        if not os.path.exists(dataset_dir):\n            os.makedirs(dataset_dir)\n        fsents = open(sent_filename, \"w\")\n        fposs = open(poss_filename, \"w\")\n        sentences = nltk.corpus.treebank.tagged_sents()\n        for sent in sentences:\n            fsents.write(\" \".join([w for w, p in sent]) + \"\\n\")\n            fposs.write(\" \".join([p for w, p in sent]) + \"\\n\")\n        fsents.close()\n        fposs.close()\n    sents, poss = [], []\n    with open(sent_filename, \"r\") as fsent:\n        for idx, line in enumerate(fsent):\n            sents.append(line.strip())\n            if num_pairs is not None and idx >= num_pairs:\n                break\n    with open(poss_filename, \"r\") as fposs:\n        for idx, line in enumerate(fposs):\n            poss.append(line.strip())\n            if num_pairs is not None and idx >= num_pairs:\n                break\n    return sents, poss\nsents, poss = download_and_read(\"./datasets\")\nassert(len(sents) == len(poss))\nprint(\"# of records: {:d}\".format(len(sents)))","metadata":{"execution":{"iopub.status.busy":"2024-08-06T18:02:12.459927Z","iopub.execute_input":"2024-08-06T18:02:12.460271Z","iopub.status.idle":"2024-08-06T18:02:14.435683Z","shell.execute_reply.started":"2024-08-06T18:02:12.460239Z","shell.execute_reply":"2024-08-06T18:02:14.434586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_and_build_vocab(texts, vocab_size=None, lower=True):\n    if vocab_size is None:\n        tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=lower)\n    else:\n        tokenizer = tf.keras.preprocessing.text.Tokenizer(\n            num_words=vocab_size+1, oov_token=\"UNK\", lower=lower)\n    tokenizer.fit_on_texts(texts)\n    if vocab_size is not None:\n        # additional workaround, see issue 8092\n        # https://github.com/keras-team/keras/issues/8092\n        tokenizer.word_index = {e:i for e, i in \n                                tokenizer.word_index.items() if \n                                i <= vocab_size+1 }\n    word2idx = tokenizer.word_index\n    idx2word = {v:k for k, v in word2idx.items()}\n    return word2idx, idx2word, tokenizer\nword2idx_s, idx2word_s, tokenizer_s = tokenize_and_build_vocab(sents, vocab_size=9000)\nword2idx_t, idx2word_t, tokenizer_t = tokenize_and_build_vocab(poss, vocab_size=38, lower=False)\nsource_vocab_size = len(word2idx_s)\ntarget_vocab_size = len(word2idx_t)\nprint(\"vocab sizes (source): {:d}, (target): {:d}\".format(\n    source_vocab_size, target_vocab_size))","metadata":{"execution":{"iopub.status.busy":"2024-08-06T18:02:14.437075Z","iopub.execute_input":"2024-08-06T18:02:14.437833Z","iopub.status.idle":"2024-08-06T18:02:14.672713Z","shell.execute_reply.started":"2024-08-06T18:02:14.437792Z","shell.execute_reply":"2024-08-06T18:02:14.671515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence_lengths = np.array([len(s.split()) for s in sents])\nprint([(p, np.percentile(sequence_lengths, p))\n       for p in [75, 80, 90, 95, 99, 100]])\n[(75, 33.0), (80, 35.0), (90, 41.0), (95, 47.0), (99, 58.0), (100, 271.0)]","metadata":{"execution":{"iopub.status.busy":"2024-08-06T18:02:14.674361Z","iopub.execute_input":"2024-08-06T18:02:14.674804Z","iopub.status.idle":"2024-08-06T18:02:14.700342Z","shell.execute_reply.started":"2024-08-06T18:02:14.674764Z","shell.execute_reply":"2024-08-06T18:02:14.699352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_seqlen = 271\n\n# convert sentences to sequence of integers\nsents_as_ints = tokenizer_s.texts_to_sequences(sents)\nsents_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n    sents_as_ints, maxlen=max_seqlen, padding=\"post\")\n\n# convert POS tags to sequence of (categorical) integers\nposs_as_ints = tokenizer_t.texts_to_sequences(poss)\nposs_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n    poss_as_ints, maxlen=max_seqlen, padding=\"post\")\nposs_as_catints = []\n\nfor p in poss_as_ints:\n    poss_as_catints.append(tf.keras.utils.to_categorical(p,num_classes=target_vocab_size+1, dtype=\"int32\"))\nposs_as_catints = tf.keras.preprocessing.sequence.pad_sequences(\n    poss_as_catints, maxlen=max_seqlen)\n\ndataset = tf.data.Dataset.from_tensor_slices(\n    (sents_as_ints, poss_as_catints))\nidx2word_s[0], idx2word_t[0] = \"PAD\", \"PAD\"\n\n# split into training, validation, and test datasets\ndataset = dataset.shuffle(10000)\ntest_size = len(sents) // 3\nval_size = (len(sents) - test_size) // 10\n\ntest_dataset = dataset.take(test_size)\nval_dataset = dataset.skip(test_size).take(val_size)\ntrain_dataset = dataset.skip(test_size + val_size)\n\n# create batches\nbatch_size = 128\ntrain_dataset = train_dataset.batch(batch_size)\nval_dataset = val_dataset.batch(batch_size)\ntest_dataset = test_dataset.batch(batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T18:02:14.701633Z","iopub.execute_input":"2024-08-06T18:02:14.701999Z","iopub.status.idle":"2024-08-06T18:02:15.436971Z","shell.execute_reply.started":"2024-08-06T18:02:14.70197Z","shell.execute_reply":"2024-08-06T18:02:15.435927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def masked_accuracy():\n    def masked_accuracy_fn(ytrue, ypred):\n        ytrue = tf.keras.backend.argmax(ytrue, axis=-1)\n        ypred = tf.keras.backend.argmax(ypred, axis=-1)\n        mask = tf.keras.backend.cast(\n            tf.keras.backend.not_equal(ypred, 0), tf.int32)\n        matches = tf.keras.backend.cast(\n            tf.keras.backend.equal(ytrue, ypred), tf.int32) * mask\n        numer = tf.keras.backend.sum(matches)\n        denom = tf.keras.backend.maximum(tf.keras.backend.sum(mask), 1)\n        accuracy = numer / denom\n        return accuracy\n    return masked_accuracy_fn","metadata":{"execution":{"iopub.status.busy":"2024-08-06T18:02:15.441754Z","iopub.execute_input":"2024-08-06T18:02:15.442116Z","iopub.status.idle":"2024-08-06T18:02:15.44893Z","shell.execute_reply.started":"2024-08-06T18:02:15.442083Z","shell.execute_reply":"2024-08-06T18:02:15.44789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class POSTaggingModel(tf.keras.Model):\n    def __init__(self, source_vocab_size, target_vocab_size,\n                 embedding_dim, max_seqlen, rnn_output_dim, **kwargs):\n        super(POSTaggingModel, self).__init__(**kwargs)\n        self.embed = tf.keras.layers.Embedding(\n            source_vocab_size, embedding_dim, input_length=max_seqlen)\n        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n        self.rnn = tf.keras.layers.Bidirectional(\n            tf.keras.layers.GRU(rnn_output_dim, return_sequences=True))\n        self.dense = tf.keras.layers.TimeDistributed(\n            tf.keras.layers.Dense(target_vocab_size))\n        self.activation = tf.keras.layers.Activation(\"softmax\")\n    def call(self, x):\n        x = self.embed(x)\n        x = self.dropout(x)\n        x = self.rnn(x)\n        x = self.dense(x)\n        x = self.activation(x)\n        return x\nembedding_dim = 128\nrnn_output_dim = 256\nmodel = POSTaggingModel(source_vocab_size, target_vocab_size,\n                        embedding_dim, max_seqlen, rnn_output_dim)\nmodel.build(input_shape=(batch_size, max_seqlen))\nmodel.summary()\nmodel.compile(\n    loss=\"categorical_crossentropy\",\n    optimizer=\"adam\",\n    metrics=[\"accuracy\", masked_accuracy()])","metadata":{"execution":{"iopub.status.busy":"2024-08-06T18:02:15.450232Z","iopub.execute_input":"2024-08-06T18:02:15.450569Z","iopub.status.idle":"2024-08-06T18:02:16.052664Z","shell.execute_reply.started":"2024-08-06T18:02:15.450541Z","shell.execute_reply":"2024-08-06T18:02:16.051744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Fine-tuning BERT model**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, BertConfig\nfrom transformers import AdamW, BertForSequenceClassification, get_linear_schedule_with_warmup\nfrom tqdm import tqdm, trange\nimport pandas as pd\nimport io\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-09-11T17:53:58.111234Z","iopub.execute_input":"2024-09-11T17:53:58.111814Z","iopub.status.idle":"2024-09-11T17:54:16.147316Z","shell.execute_reply.started":"2024-09-11T17:53:58.111778Z","shell.execute_reply":"2024-09-11T17:54:16.146484Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n!curl -L https://raw.githubusercontent.com/Denis2054/Transformers-for-NLP-2nd-Edition/master/Chapter03/in_domain_train.tsv --output \"in_domain_train.tsv\"\n!curl -L https://raw.githubusercontent.com/Denis2054/Transformers-for-NLP-2nd-Edition/master/Chapter03/out_of_domain_dev.tsv --output \"out_of_domain_dev.tsv\"","metadata":{"execution":{"iopub.status.busy":"2024-09-11T17:54:29.76135Z","iopub.execute_input":"2024-09-11T17:54:29.762032Z","iopub.status.idle":"2024-09-11T17:54:32.287514Z","shell.execute_reply.started":"2024-09-11T17:54:29.761994Z","shell.execute_reply":"2024-09-11T17:54:32.286545Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  428k  100  428k    0     0  1580k      0 --:--:-- --:--:-- --:--:-- 1580k\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 28506  100 28506    0     0   111k      0 --:--:-- --:--:-- --:--:--  111k\n","output_type":"stream"}]},{"cell_type":"code","source":"#source of dataset : https://nyu-mll.github.io/CoLA/\ndf = pd.read_csv(\"in_domain_train.tsv\", delimiter='\\t', header=None,\n                 names=['sentence_source', 'label', 'label_notes', 'sentence'])\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-11T17:54:33.491142Z","iopub.execute_input":"2024-09-11T17:54:33.492224Z","iopub.status.idle":"2024-09-11T17:54:33.524118Z","shell.execute_reply.started":"2024-09-11T17:54:33.49219Z","shell.execute_reply":"2024-09-11T17:54:33.523214Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(8551, 4)"},"metadata":{}}]},{"cell_type":"code","source":"df.sample(15)","metadata":{"execution":{"iopub.status.busy":"2024-09-11T17:54:35.44051Z","iopub.execute_input":"2024-09-11T17:54:35.441265Z","iopub.status.idle":"2024-09-11T17:54:35.466382Z","shell.execute_reply.started":"2024-09-11T17:54:35.441232Z","shell.execute_reply":"2024-09-11T17:54:35.465499Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"     sentence_source  label label_notes  \\\n3840            ks08      1         NaN   \n2877            l-93      1         NaN   \n1780            r-67      1         NaN   \n3764            ks08      1         NaN   \n3080            l-93      0           *   \n3329            l-93      1         NaN   \n4574            ks08      0           *   \n6365            d_98      1         NaN   \n6059            c_13      1         NaN   \n1308            r-67      1         NaN   \n5899            c_13      1         NaN   \n306             cj99      0          ??   \n231             cj99      0           *   \n4427            ks08      1         NaN   \n6995          sgww85      1         NaN   \n\n                                               sentence  \n3840  the school awarded a few of the girls in miss ...  \n2877                      i mixed the eggs with cream .  \n1780  willy is taller than bill by more than joe is ...  \n3764                    john gave the cds to the boys .  \n3080                            ellen conferred helen .  \n3329  on the comer of the two boulevards stood a sta...  \n4574  sam may have been being interrogating by the f...  \n6365                       a lion is usually majestic .  \n6059   if i were a rich man , i 'd buy a diamond ring .  \n1308  students who fail the final exam will be execu...  \n5899                                megan loves kevin .  \n306   those are the folks that you just solve this p...  \n231   when bill smokes , much more does susan hate h...  \n4427                 was the child running to the car ?  \n6995  pat was neither recommended for promotion nor ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_source</th>\n      <th>label</th>\n      <th>label_notes</th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3840</th>\n      <td>ks08</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>the school awarded a few of the girls in miss ...</td>\n    </tr>\n    <tr>\n      <th>2877</th>\n      <td>l-93</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>i mixed the eggs with cream .</td>\n    </tr>\n    <tr>\n      <th>1780</th>\n      <td>r-67</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>willy is taller than bill by more than joe is ...</td>\n    </tr>\n    <tr>\n      <th>3764</th>\n      <td>ks08</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>john gave the cds to the boys .</td>\n    </tr>\n    <tr>\n      <th>3080</th>\n      <td>l-93</td>\n      <td>0</td>\n      <td>*</td>\n      <td>ellen conferred helen .</td>\n    </tr>\n    <tr>\n      <th>3329</th>\n      <td>l-93</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>on the comer of the two boulevards stood a sta...</td>\n    </tr>\n    <tr>\n      <th>4574</th>\n      <td>ks08</td>\n      <td>0</td>\n      <td>*</td>\n      <td>sam may have been being interrogating by the f...</td>\n    </tr>\n    <tr>\n      <th>6365</th>\n      <td>d_98</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>a lion is usually majestic .</td>\n    </tr>\n    <tr>\n      <th>6059</th>\n      <td>c_13</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>if i were a rich man , i 'd buy a diamond ring .</td>\n    </tr>\n    <tr>\n      <th>1308</th>\n      <td>r-67</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>students who fail the final exam will be execu...</td>\n    </tr>\n    <tr>\n      <th>5899</th>\n      <td>c_13</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>megan loves kevin .</td>\n    </tr>\n    <tr>\n      <th>306</th>\n      <td>cj99</td>\n      <td>0</td>\n      <td>??</td>\n      <td>those are the folks that you just solve this p...</td>\n    </tr>\n    <tr>\n      <th>231</th>\n      <td>cj99</td>\n      <td>0</td>\n      <td>*</td>\n      <td>when bill smokes , much more does susan hate h...</td>\n    </tr>\n    <tr>\n      <th>4427</th>\n      <td>ks08</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>was the child running to the car ?</td>\n    </tr>\n    <tr>\n      <th>6995</th>\n      <td>sgww85</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>pat was neither recommended for promotion nor ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Creating sentence, label lists and adding Bert tokens\nsentences = df.sentence_source.values\n\n# Adding CLS and SEP tokens at the beginning and end of each sentence for BERT\nsentences = [\"[CLS] \" + sentence_source + \" [SEP]\" for sentence_source in sentences]\nlabels = df.label.values\n\n# Activating the BERT Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\ntokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\nprint (\"Tokenize the first sentence:\")\nprint (tokenized_texts[0])","metadata":{"execution":{"iopub.status.busy":"2024-09-11T17:54:37.751982Z","iopub.execute_input":"2024-09-11T17:54:37.752327Z","iopub.status.idle":"2024-09-11T17:54:39.963311Z","shell.execute_reply.started":"2024-09-11T17:54:37.752302Z","shell.execute_reply":"2024-09-11T17:54:39.962293Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c109941f452d478eb782f6cec1467fe6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e34af3c42c394bfbb6c60f6c976707f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5afd6959583406299c2bca2422dbe57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0cec74715d041adb1b54376fd860b9c"}},"metadata":{}},{"name":"stdout","text":"Tokenize the first sentence:\n['[CLS]', 'g', '##j', '##0', '##4', '[SEP]']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway.\n# In the original paper, the authors used a length of 512.\nMAX_LEN = 128\n\n# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n\n# Pad our input tokens\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n\n# Create attention masks\nattention_masks = []\n\n# Create a mask of 1s for each token followed by 0s for padding\nfor seq in input_ids:\n    seq_mask = [float(i>0) for i in seq]\n    attention_masks.append(seq_mask)\n    \n# Splitting data into train and validation sets\n# Use train_test_split to split our data into train and validation sets for training\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state = 2018, test_size = 0.1)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,random_state = 2018, test_size = 0.1)\n\n# Converting all the data into torch tensors\n# Torch tensors are the required datatype for our model\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)\n\n# Selecting a Batch Size and Creating and Iterator\n# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\nbatch_size = 32\n\n# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop,\n# with an iterator the entire dataset does not need to be loaded into memory\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size = batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-09-11T17:54:41.979703Z","iopub.execute_input":"2024-09-11T17:54:41.980543Z","iopub.status.idle":"2024-09-11T17:54:43.13573Z","shell.execute_reply.started":"2024-09-11T17:54:41.980504Z","shell.execute_reply":"2024-09-11T17:54:43.13491Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# BERT Model Configuration, initializing a BERT bert-base-uncased style configuration\n# Transformer Installation\ntry:\n    import transformers\nexcept:\n    print(\"Installing transformers\")\n    !pip -qq install transformers\n    \nfrom transformers import BertModel, BertConfig\nconfiguration = BertConfig()\n\n# Initializing a model from the bert-base-uncased style configuration\nmodel = BertModel(configuration)\n\n# Accessing the model configuration\nconfiguration = model.config\nprint(configuration)","metadata":{"execution":{"iopub.status.busy":"2024-09-11T17:54:45.196442Z","iopub.execute_input":"2024-09-11T17:54:45.196794Z","iopub.status.idle":"2024-09-11T17:54:46.974089Z","shell.execute_reply.started":"2024-09-11T17:54:45.196766Z","shell.execute_reply":"2024-09-11T17:54:46.973153Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"BertConfig {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.35.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loading the Hugging Face Bert uncased base model\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 2)\nmodel = nn.DataParallel(model)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-11T17:54:49.341253Z","iopub.execute_input":"2024-09-11T17:54:49.341892Z","iopub.status.idle":"2024-09-11T17:54:52.651853Z","shell.execute_reply.started":"2024-09-11T17:54:49.341857Z","shell.execute_reply":"2024-09-11T17:54:52.650909Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b40370089fbb4ef19ff0761eca8c5ca6"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DataParallel(\n  (module): BertForSequenceClassification(\n    (bert): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (dropout): Dropout(p=0.1, inplace=False)\n    (classifier): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Optimizer Grouped Parameters\n#This code is taken from:\n# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L102\n# Don't apply weight decay to any parameters whose names include these tokens.\n# (Here, the BERT doesn't have 'gamma' or 'beta' parameters, only 'bias' terms)\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.weight']\n\n# Separate the 'weight' parameters from the 'bias' parameters.\n# - For the 'weight' parameters, this specifies a 'weight_decay_rate' of 0.01.\n# - For the 'bias' parameters, the 'weight_decay_rate' is 0.0.\noptimizer_grouped_parameters = [\n    # Filter for all parameters which *don't* include 'bias', 'gamma', 'beta'.\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.1},\n    # Filter for parameters which *do* include those.\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]\n\n# Displaying a sample of the parameter_optimizer:  layer 3\nlayer_parameters = [p for n, p in model.named_parameters() if 'layer.3' in n]\n\n# Displaying names of parameters for which weight decay is not applied\nno_decay","metadata":{"execution":{"iopub.status.busy":"2024-09-11T17:54:55.131811Z","iopub.execute_input":"2024-09-11T17:54:55.132525Z","iopub.status.idle":"2024-09-11T17:54:55.143713Z","shell.execute_reply.started":"2024-09-11T17:54:55.132478Z","shell.execute_reply":"2024-09-11T17:54:55.142813Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"['bias', 'LayerNorm.weight']"},"metadata":{}}]},{"cell_type":"code","source":"# Displaying the list of the two dictionaries\nsmall_sample = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)][:2],\n     'weight_decay_rate': 0.1},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)][:2],\n     'weight_decay_rate': 0.0}\n]\n\nfor i, group in enumerate(small_sample):\n    print(f\"Group {i+1}:\")\n    print(f\"Weight decay rate: {group['weight_decay_rate']}\")\n    for j, param in enumerate(group['params']):\n        print(f\"Parameter {j+1}: {param}\")\n     ","metadata":{"execution":{"iopub.status.busy":"2024-09-11T17:54:57.72652Z","iopub.execute_input":"2024-09-11T17:54:57.727299Z","iopub.status.idle":"2024-09-11T17:54:57.983837Z","shell.execute_reply.started":"2024-09-11T17:54:57.727265Z","shell.execute_reply":"2024-09-11T17:54:57.982939Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Group 1:\nWeight decay rate: 0.1\nParameter 1: Parameter containing:\ntensor([[-0.0102, -0.0615, -0.0265,  ..., -0.0199, -0.0372, -0.0098],\n        [-0.0117, -0.0600, -0.0323,  ..., -0.0168, -0.0401, -0.0107],\n        [-0.0198, -0.0627, -0.0326,  ..., -0.0165, -0.0420, -0.0032],\n        ...,\n        [-0.0218, -0.0556, -0.0135,  ..., -0.0043, -0.0151, -0.0249],\n        [-0.0462, -0.0565, -0.0019,  ...,  0.0157, -0.0139, -0.0095],\n        [ 0.0015, -0.0821, -0.0160,  ..., -0.0081, -0.0475,  0.0753]],\n       device='cuda:0', requires_grad=True)\nParameter 2: Parameter containing:\ntensor([[ 1.7505e-02, -2.5631e-02, -3.6642e-02,  ...,  3.3437e-05,\n          6.8312e-04,  1.5441e-02],\n        [ 7.7580e-03,  2.2613e-03, -1.9444e-02,  ...,  2.8910e-02,\n          2.9753e-02, -5.3247e-03],\n        [-1.1287e-02, -1.9644e-03, -1.1573e-02,  ...,  1.4908e-02,\n          1.8741e-02, -7.3140e-03],\n        ...,\n        [ 1.7418e-02,  3.4903e-03, -9.5621e-03,  ...,  2.9599e-03,\n          4.3435e-04, -2.6949e-02],\n        [ 2.1687e-02, -6.0216e-03,  1.4736e-02,  ..., -5.6118e-03,\n         -1.2590e-02, -2.8085e-02],\n        [ 2.6413e-03, -2.3298e-02,  5.4922e-03,  ...,  1.7537e-02,\n          2.7550e-02, -7.7656e-02]], device='cuda:0', requires_grad=True)\nGroup 2:\nWeight decay rate: 0.0\nParameter 1: Parameter containing:\ntensor([0.9261, 0.8851, 0.8581, 0.8617, 0.8937, 0.8969, 0.9297, 0.9137, 0.9371,\n        0.8084, 0.7992, 0.8071, 0.9031, 0.8198, 0.9100, 0.8493, 0.8152, 0.8613,\n        0.9142, 0.8652, 0.9234, 0.8672, 0.9008, 0.8684, 0.8440, 0.8990, 0.7891,\n        0.9275, 0.8501, 0.8413, 0.9179, 0.8641, 0.9185, 0.9657, 0.8861, 0.8710,\n        0.9103, 0.8739, 0.9133, 0.8880, 0.9130, 0.9374, 0.8823, 0.8622, 0.8812,\n        0.8708, 0.8570, 0.9445, 0.9163, 0.9356, 0.9265, 0.8504, 0.9300, 0.3447,\n        0.8650, 0.8197, 0.8722, 0.8566, 0.8939, 0.8051, 0.9007, 0.8483, 0.3870,\n        0.8889, 0.8923, 0.8772, 0.8963, 0.9548, 0.8944, 0.8946, 0.9471, 0.9489,\n        0.9349, 0.7814, 0.9255, 0.7943, 0.8806, 0.3857, 0.7900, 0.8478, 0.8886,\n        0.9215, 0.9292, 0.8990, 0.7790, 0.8255, 0.8717, 0.8778, 0.9021, 0.9190,\n        0.8605, 0.8762, 0.7084, 0.8599, 0.8981, 0.8092, 0.4021, 0.7917, 0.8923,\n        0.9118, 0.9459, 0.9489, 0.8744, 0.8402, 0.8031, 0.2923, 0.9314, 0.9065,\n        0.8852, 0.8115, 0.9090, 0.8948, 0.9024, 0.9147, 0.8994, 0.8709, 0.9006,\n        0.9032, 0.8945, 0.8992, 0.8674, 0.1510, 0.8702, 0.8860, 0.6453, 0.8554,\n        0.8652, 0.7923, 0.8951, 0.8370, 0.9121, 0.8860, 0.4010, 0.8836, 0.8963,\n        0.9265, 0.8997, 0.8338, 0.9043, 0.4212, 0.9174, 0.9093, 0.9214, 0.9099,\n        0.9185, 0.1877, 0.9138, 0.8466, 0.8435, 0.8340, 0.8081, 0.9064, 0.7318,\n        0.8842, 0.9489, 0.9096, 0.9133, 0.8368, 0.9172, 0.1242, 0.9090, 0.9128,\n        0.8443, 0.9831, 0.9145, 0.9176, 0.8641, 0.8422, 0.3847, 0.8949, 0.9381,\n        0.8940, 0.8285, 0.9439, 0.9318, 0.1185, 0.8464, 0.8418, 0.8703, 0.8623,\n        0.8936, 0.9349, 0.8707, 0.9318, 0.9362, 0.9171, 0.9231, 0.8718, 0.9331,\n        0.8409, 0.8814, 0.8905, 0.8714, 0.8592, 0.8775, 0.8865, 0.9147, 0.9135,\n        0.7989, 0.8081, 0.8708, 0.9014, 0.2742, 0.8638, 0.8609, 0.8354, 0.9148,\n        0.8464, 0.9376, 0.8697, 0.8553, 0.9501, 0.8380, 0.9075, 0.8760, 0.2617,\n        0.1264, 0.9096, 0.5226, 0.8553, 0.9289, 0.8844, 0.9436, 0.8768, 0.8213,\n        0.3509, 0.8355, 0.7795, 0.9288, 0.8961, 0.8741, 0.8785, 0.8789, 0.9082,\n        0.9163, 0.8849, 0.8641, 0.8015, 0.9239, 0.7953, 0.8380, 0.8894, 0.9044,\n        0.8646, 0.8414, 0.3916, 0.8405, 0.8416, 0.8593, 0.4864, 0.8912, 0.8796,\n        0.8739, 0.8619, 0.9102, 0.8609, 0.8451, 0.8702, 0.8543, 0.8729, 0.9333,\n        0.8829, 0.9353, 0.8981, 0.7935, 0.8589, 0.9080, 0.8970, 0.9057, 0.9114,\n        0.8719, 0.8928, 0.8630, 0.8621, 0.9335, 0.9372, 0.8323, 0.8541, 0.8904,\n        0.9081, 0.8410, 0.9020, 0.9285, 0.3199, 0.8969, 0.8276, 0.8606, 0.9147,\n        0.8596, 0.9309, 0.8497, 0.4077, 0.8589, 0.8980, 0.8605, 0.8013, 0.3466,\n        0.9165, 0.8924, 0.8898, 0.8680, 0.9268, 0.9153, 0.8970, 0.7476, 0.9126,\n        0.8541, 0.8794, 0.8637, 0.8899, 0.8578, 0.3877, 0.8876, 0.8887, 0.9539,\n        0.8464, 0.9220, 0.9094, 0.8906, 0.4427, 0.8934, 0.8639, 0.8826, 0.8951,\n        0.9021, 0.9081, 0.8711, 0.9089, 0.9003, 0.8644, 0.8243, 0.4171, 0.8461,\n        0.9071, 0.8809, 0.8440, 0.8420, 0.9303, 0.9043, 0.9185, 0.7761, 0.8801,\n        0.8777, 0.8956, 0.8651, 0.3741, 0.9125, 0.8279, 0.8863, 0.9568, 0.9130,\n        0.9308, 0.9037, 0.2834, 0.8967, 0.9063, 0.8815, 0.8595, 0.8862, 0.8595,\n        0.9078, 0.8376, 0.8270, 0.8330, 0.9077, 0.8695, 0.8746, 0.8756, 0.8910,\n        0.8326, 0.8960, 0.8280, 0.9293, 0.8439, 0.8843, 0.9747, 0.9029, 0.8918,\n        0.8340, 0.9228, 0.8763, 0.5711, 0.8947, 0.9175, 0.8911, 0.8977, 0.8908,\n        0.9205, 0.9037, 0.8681, 0.8172, 0.8671, 0.8722, 0.8607, 0.9117, 0.8887,\n        0.8645, 0.8427, 0.8681, 0.9256, 0.8530, 0.6331, 0.8565, 0.8803, 0.7949,\n        0.8887, 0.9112, 0.8817, 0.8718, 0.9304, 0.9084, 0.8746, 0.9361, 0.8845,\n        0.8872, 0.8587, 0.8798, 0.8725, 0.8966, 0.3244, 0.8557, 0.7890, 0.8897,\n        0.8419, 0.8509, 0.8581, 0.8828, 0.9216, 0.9017, 0.7707, 0.9108, 0.8098,\n        0.9066, 0.8178, 0.9126, 0.9510, 0.8832, 0.8151, 0.8725, 0.8795, 0.8997,\n        0.8967, 0.8649, 0.3751, 0.8935, 0.7732, 0.9368, 0.8688, 0.8896, 0.8816,\n        0.8823, 0.9228, 0.8957, 0.9060, 0.9271, 0.9419, 0.8766, 0.8950, 0.8504,\n        0.8929, 0.8664, 0.3603, 0.9116, 0.9117, 0.8619, 0.8388, 0.8478, 0.8033,\n        0.8349, 0.8558, 0.8909, 0.9125, 0.8740, 0.8953, 0.8341, 0.8763, 0.8494,\n        0.8897, 0.8818, 0.8597, 0.9161, 0.8733, 0.3575, 0.9128, 0.8563, 0.8596,\n        0.8857, 0.8927, 0.8683, 0.8929, 0.9100, 0.8581, 0.8916, 0.9047, 0.9335,\n        0.8936, 0.3576, 0.8567, 0.9137, 0.8022, 0.8484, 0.8865, 0.8539, 0.8018,\n        0.7964, 0.8703, 0.8703, 0.8715, 0.9090, 0.9053, 0.7068, 0.8064, 0.8901,\n        0.8739, 0.8705, 0.8710, 0.8932, 0.3423, 0.8911, 0.8759, 0.9116, 0.8306,\n        0.9239, 0.8675, 0.8874, 0.9189, 0.8805, 0.9231, 0.8938, 0.0893, 0.8046,\n        0.8052, 0.9390, 0.9132, 0.8941, 0.8958, 0.9140, 0.8667, 0.8548, 0.8387,\n        0.8630, 0.1531, 0.9126, 0.8283, 0.9090, 0.3610, 0.8953, 0.8742, 0.8377,\n        0.9150, 0.9009, 0.8382, 0.9282, 0.9210, 0.9416, 0.9298, 0.8366, 0.8120,\n        0.8995, 0.9162, 0.9217, 0.8715, 0.8563, 0.9000, 0.8472, 0.8983, 0.8892,\n        0.8823, 0.8943, 0.9088, 0.8511, 0.9041, 0.8712, 0.6926, 0.9088, 0.8996,\n        0.8979, 0.3115, 0.9052, 0.8958, 0.8547, 0.9021, 0.9065, 0.8612, 0.9080,\n        0.8559, 0.9115, 0.8740, 0.8124, 0.9029, 0.9070, 0.8975, 0.9285, 0.8457,\n        0.8160, 0.8196, 0.8904, 0.8613, 0.9282, 0.8853, 0.8926, 0.9372, 0.9101,\n        0.8404, 0.8816, 0.8422, 0.9576, 0.8314, 0.9008, 0.8606, 0.9050, 0.8172,\n        0.9053, 0.8068, 0.3706, 0.8758, 0.8680, 0.9558, 0.8983, 0.8546, 0.8458,\n        0.8926, 0.9333, 0.8657, 0.9090, 0.8623, 0.8937, 0.8931, 0.8624, 0.8948,\n        0.8467, 0.8906, 0.8558, 0.8929, 0.9050, 0.9152, 0.9015, 0.9238, 0.8295,\n        0.9300, 0.8609, 0.8456, 0.8952, 0.8212, 0.9223, 0.8452, 0.8642, 0.8961,\n        0.8735, 0.8625, 0.9424, 0.9192, 0.8447, 0.8661, 0.1741, 0.8413, 0.8128,\n        0.8900, 0.9181, 0.8831, 0.8919, 0.9229, 0.9022, 0.8878, 0.9241, 0.8676,\n        0.9140, 0.8971, 0.8902, 0.8600, 0.4131, 0.8796, 0.8501, 0.8333, 0.9113,\n        0.9093, 0.8881, 0.8883, 0.9446, 0.8709, 0.9057, 0.9258, 0.8367, 0.9239,\n        0.9440, 0.8931, 0.8713, 0.9279, 0.9033, 0.9183, 0.9104, 0.8797, 0.8418,\n        0.9657, 0.8738, 0.8632, 0.8565, 0.8197, 0.8892, 0.3128, 0.9198, 0.9016,\n        0.8746, 0.8265, 0.9062, 0.8387, 0.7941, 0.8618, 0.8994, 0.8790, 0.8075,\n        0.8651, 0.7939, 0.9075, 0.8526, 0.9196, 0.8239, 0.8574, 0.9099, 0.8529,\n        0.8431, 0.8975, 0.9211, 0.9451, 0.8933, 0.9079, 0.8607, 0.7929, 0.8643,\n        0.8873, 0.8862, 0.9134, 0.9299, 0.7502, 0.8474, 0.8630, 0.9158, 0.8375,\n        0.9456, 0.8130, 0.8500, 0.9144, 0.9410, 0.8897, 0.8587, 0.8965, 0.8335,\n        0.8710, 0.8764, 0.8918, 0.9459, 0.9261, 0.8914, 0.8858, 0.9025, 0.8341,\n        0.8275, 0.9246, 0.8769, 0.7869, 0.9064, 0.8805, 0.8635, 0.9058, 0.8922,\n        0.7889, 0.8703, 0.9371], device='cuda:0', requires_grad=True)\nParameter 2: Parameter containing:\ntensor([-2.5915e-02, -1.9551e-02,  2.4239e-02,  8.9046e-02, -6.2811e-02,\n        -1.3259e-02,  1.3368e-02, -1.2601e-02,  4.4289e-03,  3.5345e-02,\n        -4.7754e-02, -1.3850e-02,  4.9743e-03, -1.0346e-01, -7.5715e-02,\n         2.6812e-02, -2.0635e-02, -4.3696e-02, -5.6391e-02, -8.6585e-02,\n        -8.4264e-03, -3.7167e-02, -4.3100e-02, -2.3440e-02,  2.0096e-02,\n        -6.8347e-03, -6.1792e-02, -4.7181e-02,  3.5398e-03, -5.6818e-03,\n        -4.0273e-02, -1.0445e-01, -3.0861e-02, -4.7370e-02, -5.1382e-02,\n         5.2509e-02, -6.0150e-03, -6.3076e-02, -3.2632e-02, -1.0294e-02,\n        -2.6134e-02,  3.2977e-02,  2.3532e-02, -3.7834e-02, -7.1279e-02,\n         1.8800e-02, -4.8075e-02, -2.5656e-02, -1.0538e-01,  4.4619e-03,\n        -2.8683e-02, -5.9236e-02,  1.2326e-02,  1.2549e-01, -5.6910e-02,\n        -7.5655e-02, -3.9333e-02,  6.9859e-03, -6.4214e-02, -4.6906e-02,\n        -2.1134e-02, -4.0144e-02, -7.7445e-02, -5.1023e-02,  2.9909e-02,\n         2.6647e-02, -4.1158e-02, -9.2278e-02, -4.4493e-02, -1.9326e-02,\n        -5.7582e-02, -3.1668e-03,  3.4696e-03, -1.6554e-02,  2.3527e-02,\n        -4.8038e-02,  9.7797e-03, -1.0442e-01, -4.8437e-02,  7.3197e-03,\n        -1.9734e-02, -4.2070e-02, -7.4608e-02, -8.5390e-04, -5.0404e-02,\n        -5.9856e-02, -4.1895e-02, -1.5177e-02, -6.5195e-02,  1.3958e-02,\n        -5.3032e-02, -2.6719e-02, -8.8763e-02, -9.3736e-02, -7.9080e-03,\n        -7.2518e-03, -6.7675e-03,  1.1923e-02, -1.0729e-03,  5.0090e-02,\n        -1.4544e-02,  9.8045e-03, -2.7189e-02, -9.0469e-03, -2.6189e-02,\n        -5.1260e-02,  6.2072e-03, -4.5944e-03, -4.2397e-02, -8.6799e-02,\n         1.0473e-01, -4.4651e-02, -1.2098e-03,  4.8902e-02, -6.6317e-02,\n        -6.5932e-02, -6.2869e-02, -4.5771e-03, -1.2409e-02, -3.4328e-02,\n        -3.1108e-02,  6.1508e-01, -5.7233e-02, -3.1616e-02,  3.6678e-02,\n        -1.2333e-02, -4.1743e-02,  7.6624e-02, -4.7894e-02,  5.1988e-03,\n        -3.1304e-02, -1.2102e-03,  2.2624e-02, -8.7661e-02,  2.3106e-02,\n        -8.2632e-03, -3.9954e-03, -2.6696e-02, -2.7406e-02,  1.2978e-01,\n        -1.6759e-03, -5.2163e-02, -4.4482e-02, -1.1419e-02, -7.6521e-02,\n         6.3131e-01, -2.7994e-02,  1.8642e-02,  1.4419e-02,  1.1243e-02,\n        -1.5756e-02, -3.2812e-03,  7.8419e-02, -2.1316e-02, -1.7731e-02,\n        -1.7104e-02, -2.1411e-02,  2.1579e-03,  7.5490e-03, -1.7664e-01,\n        -6.4108e-04, -4.8799e-02, -1.0787e-02,  2.0321e-02, -2.7500e-02,\n        -1.5150e-02, -2.5434e-02, -1.0745e-01,  1.8762e-01,  2.7139e-02,\n        -5.7344e-02, -5.7125e-02, -7.7040e-02, -6.8253e-04, -2.8963e-02,\n        -5.5552e-01, -7.3063e-02, -2.8202e-02, -9.1884e-02, -5.5053e-02,\n        -9.9699e-03, -2.3530e-02,  2.2338e-02,  1.1792e-02, -3.9402e-02,\n        -4.2718e-02, -7.0445e-02, -8.4193e-02, -1.5879e-02, -3.8651e-02,\n        -9.6151e-03,  2.3536e-02, -2.7933e-02,  4.9370e-02, -3.2360e-03,\n         4.4905e-03, -1.4351e-02, -4.7481e-02,  5.6372e-02,  2.5966e-02,\n        -2.2563e-02, -1.2718e-01, -4.2220e-02, -1.7046e-02,  3.3547e-02,\n        -6.6450e-02, -5.1126e-02, -1.2893e-02,  4.5700e-02, -5.0845e-02,\n        -4.1066e-03,  1.1188e-02, -9.1059e-02, -3.1550e-02, -3.3693e-02,\n        -8.1856e-02, -8.6929e-02, -4.9886e-02, -3.7611e-02, -7.4728e-02,\n        -8.2650e-02, -1.6736e-03, -3.9741e-02, -4.0385e-02,  2.5962e-02,\n        -1.1098e-01, -5.4296e-02, -5.1232e-03, -4.8667e-02,  3.3547e-02,\n        -7.9387e-03,  1.7725e-02,  4.1061e-02,  2.0418e-02,  8.8616e-03,\n        -2.1618e-02, -4.0198e-02, -1.0598e-01, -3.6634e-02,  1.2662e-02,\n        -2.1713e-02,  2.8119e-03, -3.4493e-02, -5.1686e-03,  1.8638e-02,\n        -1.2686e-01,  8.6229e-03,  2.7526e-02, -4.0473e-02,  1.8166e-02,\n        -5.8837e-03, -1.6472e-02,  1.7345e-02, -9.9164e-02, -5.7288e-02,\n        -5.2321e-02,  1.6784e-02, -4.9069e-02, -4.7374e-02, -5.9768e-02,\n        -3.3697e-02, -2.1515e-02, -6.7439e-02, -2.5190e-03, -4.5390e-02,\n        -6.3062e-02, -6.6929e-03,  7.1754e-03, -1.1063e-02, -6.1783e-03,\n        -3.5333e-04, -3.5153e-02, -4.2905e-02, -1.4619e-02,  3.9847e-02,\n        -2.7761e-02, -7.3141e-02, -1.0278e-02, -2.2473e-02, -8.3281e-03,\n        -4.7029e-02, -3.7808e-02, -1.4231e-03, -1.5028e-01,  1.4308e-02,\n         1.9772e-02, -6.9576e-03, -8.7298e-03, -3.0035e-02, -2.1216e-02,\n        -1.8634e-02, -1.8011e-02, -1.8625e-02, -7.3722e-02, -1.1419e-02,\n        -5.4381e-02, -7.2602e-02, -5.6164e-02,  4.9254e-03,  2.8565e-02,\n        -3.2194e-02,  3.3299e-02,  9.2594e-03, -1.4165e-02,  6.2481e-03,\n        -9.1849e-02, -6.1673e-02, -1.9788e-02,  8.0208e-01,  3.8315e-03,\n         7.3194e-03, -2.7890e-03, -4.5539e-02,  3.4170e-02, -5.8225e-02,\n         4.7824e-02, -3.0348e-02, -9.3666e-03, -2.2191e-02, -4.2016e-02,\n        -3.2741e-02, -4.6574e-02, -3.0170e-02,  1.2240e-02, -4.0513e-02,\n        -5.1817e-02, -4.1886e-02, -6.0936e-02, -1.6476e-02, -4.5148e-02,\n        -5.2216e-02, -8.4434e-02,  4.1617e-02,  1.5191e-02, -6.4686e-03,\n        -7.0176e-02, -4.5118e-02, -1.1611e-03, -2.9325e-02,  1.9957e-02,\n        -3.3062e-02, -5.7241e-03, -3.4643e-02, -1.8349e-02, -3.8300e-02,\n         4.2363e-02, -2.9268e-03,  1.6553e-02, -2.5765e-02, -5.7496e-02,\n         2.9435e-02, -1.9588e-02, -5.3201e-02, -1.5394e-01,  6.0334e-03,\n         3.9881e-02,  6.9188e-03, -6.4305e-02, -5.9645e-03, -5.3314e-02,\n        -3.7108e-02, -3.0492e-02, -5.4900e-02, -1.3776e-02, -4.4202e-02,\n        -1.3504e-02,  1.9884e-03, -6.2694e-02, -9.7083e-03, -2.9480e-02,\n        -4.7304e-02, -2.8082e-02, -5.3774e-02, -1.5157e-02,  3.7363e-02,\n        -7.8915e-03, -1.2612e-02, -3.1819e-02,  2.9497e-02, -6.3238e-02,\n        -4.4190e-02,  8.2589e-02,  3.4296e-03, -5.5870e-02, -1.4455e-03,\n        -4.9274e-02, -3.3716e-02, -1.6712e-03, -7.8818e-02,  1.3456e-04,\n        -4.2685e-02, -5.5039e-02, -8.0704e-04, -5.3792e-02, -4.3908e-02,\n        -4.2086e-02, -2.0246e-02, -9.8204e-03,  1.1588e-02, -5.2375e-03,\n        -1.0368e-01,  8.1678e-02,  2.4802e-02, -2.1297e-02, -9.7985e-03,\n        -5.9728e-02,  1.5593e-02, -2.7538e-02, -7.1796e-02, -2.6682e-02,\n        -9.7993e-03, -6.1281e-03, -1.3567e-02,  4.6011e-02, -3.6998e-02,\n        -2.9911e-02, -3.7790e-03,  6.2445e-02, -4.7371e-02, -2.5641e-02,\n        -3.3513e-02,  3.3076e-02, -3.4258e-03, -6.4098e-02,  6.2438e-03,\n        -2.5948e-03, -2.0595e-02,  5.8018e-03, -1.6952e-02, -3.1340e-02,\n         9.2013e-03,  4.3807e-03, -2.9900e-02, -4.6831e-02, -3.2926e-02,\n        -4.3346e-02, -5.5193e-02, -3.0171e-02, -4.6218e-02,  4.4061e-03,\n        -9.8874e-02, -4.5176e-02, -1.1195e-02, -2.7977e-02, -7.3573e-02,\n         1.1170e-02,  3.1528e-03, -3.4605e-02,  3.5081e-02, -2.1169e-02,\n        -8.5231e-02, -4.1800e-02, -7.9768e-03, -3.4629e-02, -5.7468e-02,\n        -4.7183e-02, -2.8348e-02, -3.6573e-02, -2.6794e-03, -1.7779e-02,\n        -1.4600e-02, -1.0303e-01,  3.5081e-03, -2.5528e-02, -4.9287e-02,\n         3.8913e-02, -5.2635e-02, -9.3173e-02, -4.1900e-02, -4.4379e-03,\n        -4.4030e-02, -9.7478e-02, -2.3247e-03, -5.1886e-02, -1.7192e-02,\n        -1.0453e-01, -4.2942e-02,  1.8478e-02,  1.2559e-04, -1.8215e-02,\n        -2.0113e-02, -4.5295e-02, -1.4194e-01, -3.1605e-02, -6.3404e-02,\n        -3.7623e-02, -6.0113e-02, -1.3319e-02,  1.1182e-02, -1.3580e-02,\n        -2.4558e-02,  2.4573e-02, -6.4949e-02,  3.2738e-02,  2.5244e-03,\n        -4.8500e-02, -2.2500e-01, -2.6963e-02, -1.0417e-02, -4.8178e-02,\n        -5.7472e-02,  7.4324e-03, -3.0408e-02,  4.2802e-02,  2.3494e-02,\n         7.9873e-02, -3.8498e-03,  3.8484e-03, -2.9699e-02,  1.0604e-02,\n        -3.2911e-02,  5.0820e-02, -5.2097e-02, -1.1356e-02, -1.6708e-02,\n         8.7562e-03, -2.1074e-03, -7.0831e-02,  4.1871e-03, -6.0861e-02,\n         9.9281e-03, -1.7498e-02, -4.1576e-02, -4.8107e-02, -3.5936e-04,\n        -3.7300e-02, -3.3615e-02,  9.3065e-03, -2.8743e-02,  3.3892e-01,\n         1.5589e-02, -1.2075e-02,  6.6061e-03, -5.2829e-02,  1.5670e-03,\n         4.6090e-03, -1.8197e-02, -1.4730e-02, -4.7913e-02, -5.0891e-02,\n        -4.0786e-02,  5.8770e-01, -6.4010e-02, -3.2896e-02, -4.6990e-02,\n         3.4083e-02, -5.3831e-02,  8.2924e-03, -2.8368e-02, -7.9581e-02,\n        -3.3436e-02, -1.5637e-02, -4.1197e-02, -1.0807e-02, -4.7561e-02,\n        -2.3212e-03, -7.9414e-02, -6.0072e-02, -4.9086e-02,  2.9513e-02,\n        -2.1450e-02, -5.3149e-02, -3.6076e-02, -1.6571e-03, -6.2381e-02,\n        -6.6562e-02, -2.9433e-02, -4.5580e-02,  1.0057e-01, -4.5016e-02,\n        -3.4638e-02,  2.1939e-02, -3.1183e-02,  3.4385e-02, -5.1813e-02,\n        -8.3614e-02, -5.7168e-02,  1.2340e-03, -2.0756e-02, -2.6120e-02,\n        -4.4349e-02,  2.2629e-02, -3.6499e-02, -7.4934e-02, -5.3121e-02,\n        -3.3120e-02, -2.8963e-02, -7.9662e-02, -8.9498e-03, -4.6545e-03,\n        -1.6556e-02,  2.1857e-02, -2.8907e-02, -3.7812e-02, -5.4892e-02,\n        -3.4956e-02, -4.2588e-02, -7.1425e-02,  1.5524e-02, -3.1012e-02,\n        -6.1514e-02,  2.8281e-04, -1.8246e-02, -5.7785e-02, -7.1134e-02,\n        -1.8365e-03, -5.5846e-02,  1.3763e-02,  1.6217e-02, -3.6842e-02,\n         8.6093e-03, -2.2123e-02, -3.4059e-02,  1.3480e-02,  2.1419e-02,\n        -5.8765e-02, -2.1433e-02,  1.2538e-02,  2.3434e-02, -5.7161e-02,\n        -4.8857e-02, -3.1402e-02, -3.3618e-02,  1.5734e-02, -1.7967e-02,\n         1.1478e-02,  1.1616e-02, -5.8076e-02, -3.0479e-02, -7.3423e-03,\n        -1.0336e-02, -6.3839e-02, -6.6791e-02, -1.2612e-02, -3.2404e-02,\n        -6.7228e-02, -5.3619e-02, -2.7398e-02, -7.7866e-02,  5.1737e-02,\n        -2.5613e-02,  1.1497e-02,  1.3377e-02, -5.3409e-02, -4.9095e-02,\n        -2.2944e-02,  2.5084e-03, -2.5901e-02, -5.7878e-02, -6.1535e-02,\n        -1.7601e-02,  3.0494e-02,  1.9008e-04, -8.8127e-02,  4.1211e-01,\n        -5.1857e-02, -5.3548e-02,  1.5308e-02, -1.1714e-01, -2.2206e-03,\n        -1.2477e-03,  2.6556e-02, -3.5507e-02, -5.7006e-02,  3.4786e-02,\n        -4.6205e-03, -7.0835e-02, -2.1287e-02, -1.0036e-01, -8.3618e-03,\n        -1.0289e-01, -4.1975e-02, -3.5961e-05, -7.5675e-02,  1.9396e-02,\n        -3.7410e-02,  4.2746e-02,  5.0878e-03,  1.1096e-02,  1.6870e-02,\n        -4.5282e-02, -3.0101e-02,  4.7371e-03, -6.1676e-02,  4.0941e-02,\n        -3.1624e-02, -9.7934e-03,  1.2703e-02, -1.4288e-02, -8.3961e-02,\n        -7.7898e-02, -6.4819e-02, -3.2725e-02, -1.6302e-02, -2.3457e-02,\n        -1.1265e-02, -6.2687e-02, -7.6259e-02, -2.2835e-02, -2.5439e-02,\n        -2.4275e-02, -4.0159e-02,  1.8949e-02, -5.8023e-02,  9.1777e-02,\n        -6.6573e-02, -2.8843e-02, -1.1038e-01,  1.0182e-02,  5.3695e-02,\n        -5.3867e-02, -7.2295e-03,  1.9030e-02, -1.3369e-02, -6.8040e-02,\n        -8.5284e-03, -1.7947e-03, -4.6275e-02,  1.1811e-02, -2.4199e-02,\n        -1.1963e-02, -4.9817e-02, -3.2041e-02, -3.5187e-02, -1.6900e-02,\n         5.3006e-02, -4.8574e-02, -5.8965e-03,  4.1262e-02, -4.1019e-02,\n        -1.0224e-02, -4.6405e-02,  2.5255e-02, -9.2840e-03, -5.2485e-02,\n        -4.3174e-02, -5.2193e-02,  1.9306e-02,  4.4194e-02, -6.9535e-02,\n         2.6020e-02, -4.7134e-02, -4.4771e-02,  2.4170e-02, -3.9867e-02,\n        -2.1866e-02, -4.8301e-02, -1.4754e-02,  1.3661e-03, -1.0784e-01,\n        -3.0805e-02, -2.7660e-02, -4.8102e-02, -9.1866e-03,  7.9246e-03,\n        -7.7543e-02,  4.5545e-02, -2.2095e-02,  1.2343e-02, -4.7419e-02,\n        -3.0890e-02, -4.7592e-02, -3.6342e-02,  6.3097e-04, -4.9002e-02,\n        -2.9923e-02,  2.1318e-02,  3.0553e-02], device='cuda:0',\n       requires_grad=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"# the hyperparameters for the training loop\\\n# optimizer = BertAdam(optimizer_grouped_parameters,\n#                      lr=2e-5,\n#                      warmup=.1)\n\n# Number of training epochs (authors recommend between 2 and 4)\nepochs = 4\n\noptimizer = torch.optim.AdamW(optimizer_grouped_parameters,\n                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                  )\n# Total number of training steps is number of batches * number of epochs.\n# `train_dataloader` contains batched data so `len(train_dataloader)` gives\n# us the number of batches.\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer,\n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","metadata":{"execution":{"iopub.status.busy":"2024-09-11T17:55:08.031869Z","iopub.execute_input":"2024-09-11T17:55:08.032484Z","iopub.status.idle":"2024-09-11T17:55:08.038612Z","shell.execute_reply.started":"2024-09-11T17:55:08.032451Z","shell.execute_reply":"2024-09-11T17:55:08.037732Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#Creating the Accuracy Measurement Function\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","metadata":{"execution":{"iopub.status.busy":"2024-09-11T17:55:10.168985Z","iopub.execute_input":"2024-09-11T17:55:10.169341Z","iopub.status.idle":"2024-09-11T17:55:10.17479Z","shell.execute_reply.started":"2024-09-11T17:55:10.16931Z","shell.execute_reply":"2024-09-11T17:55:10.173396Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# the training loop\nt = []\n\n# Store our loss and accuracy for plotting\ntrain_loss_set = []\n\n# trange is a tqdm wrapper around the normal python range\nfor _ in trange(epochs, desc=\"Epoch\"):\n    # Training\n    # Set our model to training mode (as opposed to evaluation mode)\n    model.train()\n\n    # Tracking variables\n    tr_loss = 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n\n    # Train the data for one epoch\n    for step, batch in enumerate(train_dataloader):\n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        # Clear out the gradients (by default they accumulate)\n        optimizer.zero_grad()\n        # Forward pass\n        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n        loss = outputs['loss']\n        train_loss_set.append(loss.item())\n        # Backward pass\n        loss.backward()\n        # Update parameters and take a step using the computed gradient\n        optimizer.step()\n\n        # Update the learning rate.\n        scheduler.step()\n\n        # Update tracking variables\n        tr_loss += loss.item()\n        nb_tr_examples += b_input_ids.size(0)\n        nb_tr_steps += 1\n\nprint(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n\n  # Validation\n\n# Put model in evaluation mode to evaluate loss on the validation set\nmodel.eval()\n\n# Tracking variables\neval_loss, eval_accuracy = 0, 0\nnb_eval_steps, nb_eval_examples = 0, 0\n\n# Evaluate data for one epoch\nfor batch in validation_dataloader:\n    # Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n    with torch.no_grad():\n        # Forward pass, calculate logit predictions\n        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n\n    # Move logits and labels to CPU\n    logits = logits['logits'].detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n\n    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n\n    eval_accuracy += tmp_eval_accuracy\n    nb_eval_steps += 1\n\nprint(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))","metadata":{"execution":{"iopub.status.busy":"2024-09-11T17:55:11.582568Z","iopub.execute_input":"2024-09-11T17:55:11.583409Z","iopub.status.idle":"2024-09-11T17:55:13.845176Z","shell.execute_reply.started":"2024-09-11T17:55:11.583376Z","shell.execute_reply":"2024-09-11T17:55:13.844028Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Epoch:   0%|          | 0/4 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nEpoch:   0%|          | 0/4 [00:01<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(b_input_ids, token_type_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, attention_mask\u001b[38;5;241m=\u001b[39mb_input_mask, labels\u001b[38;5;241m=\u001b[39mb_labels)\n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 28\u001b[0m train_loss_set\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 2 elements cannot be converted to Scalar"],"ename":"RuntimeError","evalue":"a Tensor with 2 elements cannot be converted to Scalar","output_type":"error"}]},{"cell_type":"markdown","source":"**Training evaluation**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\nplt.title(\"Training loss\")\nplt.xlabel(\"Batch\")\nplt.ylabel(\"Loss\")\nplt.plot(train_loss_set)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-11T17:55:17.268929Z","iopub.execute_input":"2024-09-11T17:55:17.269302Z","iopub.status.idle":"2024-09-11T17:55:17.562731Z","shell.execute_reply.started":"2024-09-11T17:55:17.26927Z","shell.execute_reply":"2024-09-11T17:55:17.561871Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x800 with 1 Axes>","image/png":"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"},"metadata":{}}]}]}