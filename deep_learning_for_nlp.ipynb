{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":796646,"sourceType":"datasetVersion","datasetId":19136},{"sourceId":1304644,"sourceType":"datasetVersion","datasetId":754810}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aleksandrmorozov123/deep-learning-for-nlp?scriptVersionId=186726953\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-26T18:02:37.434287Z","iopub.execute_input":"2024-03-26T18:02:37.434577Z","iopub.status.idle":"2024-03-26T18:02:37.793831Z","shell.execute_reply.started":"2024-03-26T18:02:37.434542Z","shell.execute_reply":"2024-03-26T18:02:37.792924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checking statistics of the Corpus**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport pandas as pd\n\n# we get first 10 000 values for fast running\nratings = pd.read_csv ('/kaggle/input/massive-stock-news-analysis-db-for-nlpbacktests/raw_analyst_ratings.csv')[0:10000]\nratings.info()","metadata":{"execution":{"iopub.status.busy":"2024-03-26T18:02:37.795649Z","iopub.execute_input":"2024-03-26T18:02:37.796424Z","iopub.status.idle":"2024-03-26T18:02:47.695395Z","shell.execute_reply.started":"2024-03-26T18:02:37.796389Z","shell.execute_reply":"2024-03-26T18:02:47.694398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# comparing the text of two selected ratings\nprint (repr(ratings.iloc[3399]['headline'][0:300]))\nprint (repr(ratings.iloc[5487]['headline'][0:300]))","metadata":{"execution":{"iopub.status.busy":"2024-03-26T18:02:47.696556Z","iopub.execute_input":"2024-03-26T18:02:47.696923Z","iopub.status.idle":"2024-03-26T18:02:47.702753Z","shell.execute_reply.started":"2024-03-26T18:02:47.696895Z","shell.execute_reply":"2024-03-26T18:02:47.701749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ignore spaces after the stop words\nimport re\nratings [\"paragraphs\"] = ratings [\"headline\"].map (lambda text: re.split ('[.?!]\\s*\\n', text))\nratings ['number_of_paragraphs'] = ratings [\"paragraphs\"].map (len)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T18:02:47.705555Z","iopub.execute_input":"2024-03-26T18:02:47.705879Z","iopub.status.idle":"2024-03-26T18:02:47.75747Z","shell.execute_reply.started":"2024-03-26T18:02:47.705841Z","shell.execute_reply":"2024-03-26T18:02:47.756829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preparations**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport sklearn\nimport spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom spacy.lang.de.stop_words import STOP_WORDS\n\ntfidf_text_vectorizer = TfidfVectorizer(stop_words=list(STOP_WORDS))\nvectors_text = tfidf_text_vectorizer.fit_transform (ratings ['headline'])\nvectors_text.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-26T18:02:47.758466Z","iopub.execute_input":"2024-03-26T18:02:47.758722Z","iopub.status.idle":"2024-03-26T18:02:54.357232Z","shell.execute_reply.started":"2024-03-26T18:02:47.758699Z","shell.execute_reply":"2024-03-26T18:02:54.35625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# flatten the paragraphs keeping the sentiment\nparagraph_df = pd.DataFrame ([{'headline': paragraph, 'publisher': publisher}\n                             for paragraphs, publisher in \\\n                             zip (ratings ['paragraphs'], ratings ['publisher'])\n                             for paragraph in paragraphs if paragraph])\ntfidf_para_vectorizer = TfidfVectorizer(stop_words=list(STOP_WORDS))\ntfidf_para_vectors = tfidf_para_vectorizer.fit_transform (paragraph_df ['headline'])\ntfidf_para_vectors.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-26T18:02:54.358501Z","iopub.execute_input":"2024-03-26T18:02:54.35913Z","iopub.status.idle":"2024-03-26T18:02:54.589289Z","shell.execute_reply.started":"2024-03-26T18:02:54.359089Z","shell.execute_reply":"2024-03-26T18:02:54.588455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Nonnegative matrix factorization** - $ V \\approx W \\cdot H $","metadata":{}},{"cell_type":"code","source":"# import required library\nfrom sklearn.decomposition import NMF\n\nnmf_text_model = NMF (n_components = 10, random_state = 42)\nW_text_matrix = nmf_text_model.fit_transform (vectors_text)\nH_text_matrix = nmf_text_model.components_\n\n# define a function for outputtin a summary\ndef display_topics (model, features, no_top_words=5):\n    for topic, word_vector in enumerate (nmf_text_model.components_):\n        total = word_vector.sum ()\n        largest = word_vector.argsort ()[::-1]  # invert sort order\n        print (\"\\ntopic %02d\" % topic)\n        for i in range (0, no_top_words):\n            print (\"  %s (%2.2f)\" % (features [largest [i]],\n                                    word_vector [largest[i]] * 100.0/total))\n            \n# calling the function\ndisplay_topics (nmf_text_model, tfidf_text_vectorizer.get_feature_names_out())","metadata":{"execution":{"iopub.status.busy":"2024-03-26T18:02:54.59071Z","iopub.execute_input":"2024-03-26T18:02:54.591129Z","iopub.status.idle":"2024-03-26T18:02:55.143506Z","shell.execute_reply.started":"2024-03-26T18:02:54.591095Z","shell.execute_reply":"2024-03-26T18:02:55.142551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# normalizing topics\nW_text_matrix.sum (axis=0)/W_text_matrix.sum()*100.0","metadata":{"execution":{"iopub.status.busy":"2024-03-26T18:02:55.144961Z","iopub.execute_input":"2024-03-26T18:02:55.145565Z","iopub.status.idle":"2024-03-26T18:02:55.156084Z","shell.execute_reply.started":"2024-03-26T18:02:55.14553Z","shell.execute_reply":"2024-03-26T18:02:55.154633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create a topic model for paragraphs using NMF**","metadata":{}},{"cell_type":"code","source":"nmf_para_model = NMF (n_components = 10, random_state = 42)\nW_para_matrix = nmf_para_model.fit_transform (tfidf_para_vectors)\nH_para_matrix = nmf_para_model.components_\n\ndisplay_topics (nmf_para_model, tfidf_para_vectorizer.get_feature_names_out ())","metadata":{"execution":{"iopub.status.busy":"2024-03-26T18:02:55.157563Z","iopub.execute_input":"2024-03-26T18:02:55.158282Z","iopub.status.idle":"2024-03-26T18:02:55.485263Z","shell.execute_reply.started":"2024-03-26T18:02:55.158245Z","shell.execute_reply":"2024-03-26T18:02:55.483951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Latent semantic analysis with singular value decomposition** - any $ m \\times n $ matrix V can be decomposed as follows\n$V = U \\cdot \\Sigma \\cdot V^* $","metadata":{}},{"cell_type":"code","source":"# import required module\nfrom sklearn.decomposition import TruncatedSVD\n\nsvd_para_model = TruncatedSVD (n_components = 10, random_state = 42)\nW_svd_para_matrix = svd_para_model.fit_transform (tfidf_para_vectors)\nH_svd_para_matrix = svd_para_model.components_\n\ndisplay_topics (svd_para_model, tfidf_para_vectorizer.get_feature_names_out ())","metadata":{"execution":{"iopub.status.busy":"2024-03-26T18:02:55.491276Z","iopub.execute_input":"2024-03-26T18:02:55.491844Z","iopub.status.idle":"2024-03-26T18:02:55.72408Z","shell.execute_reply.started":"2024-03-26T18:02:55.491782Z","shell.execute_reply":"2024-03-26T18:02:55.723127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Latent Dirichlet Allocation**","metadata":{}},{"cell_type":"code","source":"# import required modules\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\ncount_para_vectorizer = CountVectorizer (stop_words=list(STOP_WORDS))\ncount_para_vectors = count_para_vectorizer.fit_transform (paragraph_df ['headline'])\n\nlda_para_model = LatentDirichletAllocation (n_components = 10, random_state = 42)\nW_lda_para_matrix = lda_para_model.fit_transform (count_para_vectors)\nH_lda_para_matrix = lda_para_model.components_\n\ndisplay_topics (lda_para_model, tfidf_para_vectorizer.get_feature_names_out ())","metadata":{"execution":{"iopub.status.busy":"2024-03-26T18:02:55.72629Z","iopub.execute_input":"2024-03-26T18:02:55.727692Z","iopub.status.idle":"2024-03-26T18:03:19.87223Z","shell.execute_reply.started":"2024-03-26T18:02:55.727657Z","shell.execute_reply":"2024-03-26T18:03:19.87132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create Word Clouds to display and compare topic models**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\ndef wordcloud_topics (model, features, no_top_words = 40):\n    for topic, words in enumerate (model.components_):\n        size = {}\n        largest = words.argsort ()[::-1]  # invert sort order\n        for i in range (0, no_top_words):\n            size [features [largest [i]]] = abs (words [largest [i]])\n        wc = WordCloud (background_color = \"white\", max_words = 100,\n                       width = 960, height = 540)\n        wc.generate_from_frequencies (size)\n        plt.figure (figsize = (12, 12))\n        plt.imshow (wc, interpolation = 'bilinear')\n        plt.axis ('off')\n        \n# compare NMF and LDA model\nwordcloud_topics (nmf_para_model, tfidf_para_vectorizer.get_feature_names_out())\nwordcloud_topics (lda_para_model, count_para_vectorizer.get_feature_names_out ())","metadata":{"execution":{"iopub.status.busy":"2024-03-26T18:03:19.873493Z","iopub.execute_input":"2024-03-26T18:03:19.873865Z","iopub.status.idle":"2024-03-26T18:03:33.94835Z","shell.execute_reply.started":"2024-03-26T18:03:19.873831Z","shell.execute_reply":"2024-03-26T18:03:33.947413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Building a neural network using Pytorch**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport torch\nimport torch.nn as nn\n\nx = [[2, 5], [7, 9], [4, 8], [6, 9]]\ny = [[4], [9], [12], [17]]\n\nX = torch.tensor (x).float ()\nY = torch.tensor (y).float ()\n\ndevice = 'cuda' if torch.cuda.is_available () else 'cpu'\nX = X.to(device)\nY = Y.to(device)\n\nclass MyNeuralNet (nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.input_to_hidden_layer = nn.Linear (2, 8)\n        self.hidden_layer_activation = nn.ReLU()\n        self.hidden_to_output_layer = nn.Linear (8, 1)\n    def forward (self, x):\n        x = self.input_to_hidden_layer (x)\n        x = self.hidden_layer_activation (x)\n        x = self.hidden_to_output_layer (x)\n        return x\n    \nmynet = MyNeuralNet().to(device)\nloss_func = nn.MSELoss()\n\n_Y = mynet(X)\nloss_value = loss_func (_Y, Y)\nprint (loss_value)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T18:03:33.949582Z","iopub.execute_input":"2024-03-26T18:03:33.949905Z","iopub.status.idle":"2024-03-26T18:03:34.463753Z","shell.execute_reply.started":"2024-03-26T18:03:33.949877Z","shell.execute_reply":"2024-03-26T18:03:34.462838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.optim import SGD\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nopt = SGD (mynet.parameters(), lr = 0.001)\n\nloss_history = []\nfor _ in range(50):\n    opt.zero_grad()\n    loss_value = loss_func (mynet (X), Y)\n    loss_value.backward ()\n    opt.step ()\n    loss_history.append (loss_value.item())\n    \nplt.plot(loss_history)\nplt.title ('Loss variation over increasing epochs')\nplt.xlabel ('epochs')\nplt.ylabel ('loss value')","metadata":{"execution":{"iopub.status.busy":"2024-03-26T18:03:34.464973Z","iopub.execute_input":"2024-03-26T18:03:34.465289Z","iopub.status.idle":"2024-03-26T18:03:34.899426Z","shell.execute_reply.started":"2024-03-26T18:03:34.465263Z","shell.execute_reply":"2024-03-26T18:03:34.898499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Resnet block architecture**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nclass ResLayer (nn.Module):\n    def __init__ (self, ni, no, kernel_size, stride=1):\n        super (ResLayer, self).__init__()\n        padding = kernel_size - 2\n        self_conv = nn.Sequential (\n        nn.Conv2d (ni, no, kernel_size, stride,\n                  padding = padding),\n        nn.ReLU ())\n        \n    def forward (self, x):\n        return self.conv (x) + x","metadata":{"execution":{"iopub.status.busy":"2024-03-28T18:00:28.975965Z","iopub.execute_input":"2024-03-28T18:00:28.976336Z","iopub.status.idle":"2024-03-28T18:00:28.982829Z","shell.execute_reply.started":"2024-03-28T18:00:28.976304Z","shell.execute_reply":"2024-03-28T18:00:28.981891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nfrom torchvision import transforms,models,datasets\n!pip install torch_summary\nfrom torchsummary import summary\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmodel = models.vgg16(pretrained=True).to(device)\nsummary(model, torch.zeros(1,3,224,224))","metadata":{"execution":{"iopub.status.busy":"2024-03-28T18:04:16.28268Z","iopub.execute_input":"2024-03-28T18:04:16.283054Z","iopub.status.idle":"2024-03-28T18:04:29.278244Z","shell.execute_reply.started":"2024-03-28T18:04:16.283023Z","shell.execute_reply":"2024-03-28T18:04:29.277221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-03-28T18:06:40.084292Z","iopub.execute_input":"2024-03-28T18:06:40.085222Z","iopub.status.idle":"2024-03-28T18:06:40.092153Z","shell.execute_reply.started":"2024-03-28T18:06:40.085182Z","shell.execute_reply":"2024-03-28T18:06:40.091224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**RNN with TensorFlow**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport re\n\ndef download_and_read (urls):\n    texts = []\n    for i, url in enumerate (urls):\n        p = tf.keras.utils.get_file (\"ex1-{:d}.txt\".format (i), url, cache_dir = \".\")\n        text = open (p, \"r\").read ()\n        # remove byte order mark\n        text = text.replace (\"\\ufeff\", \"\")\n        # remove new lines\n        text = text.replace ('\\n', ' ')\n        text = re.sub (r'\\s+', \" \", text)\n        # add it to the list\n        texts.extend (text)\n    return texts\n\ntexts = download_and_read ([\"http://www.gutenberg.org/cache/epub/28885/pg28885.txt\",\n\"https://www.gutenberg.org/files/12/12-0.txt\"])","metadata":{"execution":{"iopub.status.busy":"2024-07-03T18:26:23.811849Z","iopub.execute_input":"2024-07-03T18:26:23.812453Z","iopub.status.idle":"2024-07-03T18:26:24.575368Z","shell.execute_reply.started":"2024-07-03T18:26:23.81242Z","shell.execute_reply":"2024-07-03T18:26:24.574429Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Downloading data from https://www.gutenberg.org/files/12/12-0.txt\n196464/196464 [==============================] - 0s 2us/step\n","output_type":"stream"}]}]}