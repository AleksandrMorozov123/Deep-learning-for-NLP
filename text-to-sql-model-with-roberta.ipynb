{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aleksandrmorozov123/text-to-sql-model-with-roberta?scriptVersionId=191969563\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create a Text-to-SQL from scratch - beging with first step - training a tokenizer and pretraining transformer**","metadata":{}},{"cell_type":"code","source":"# Installing Hugging Face Transformers\n!pip uninstall -y tensorflow\n# Install 'transformers' from master\n!pip install git+https://github.com/huggingface/transformers\n!pip list | grep -E 'transformers|tokenizers'","metadata":{"execution":{"iopub.status.busy":"2024-08-10T16:11:53.286114Z","iopub.execute_input":"2024-08-10T16:11:53.286535Z","iopub.status.idle":"2024-08-10T16:13:24.930696Z","shell.execute_reply.started":"2024-08-10T16:11:53.286503Z","shell.execute_reply":"2024-08-10T16:13:24.929059Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Found existing installation: tensorflow 2.15.0\nUninstalling tensorflow-2.15.0:\n  Successfully uninstalled tensorflow-2.15.0\nCollecting git+https://github.com/huggingface/transformers\n  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-pa9gvyn0\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-pa9gvyn0\n  Resolved https://github.com/huggingface/transformers to commit 48101cf8d127bbf22d751c7df118a6ce357e2e27\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.23.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.45.0.dev0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (2024.7.4)\nBuilding wheels for collected packages: transformers\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for transformers: filename=transformers-4.45.0.dev0-py3-none-any.whl size=9503295 sha256=0a1216dfe3aa2001170c953bccd9a95a99fa45fccf762283dc11a3c41dc9f8cf\n  Stored in directory: /tmp/pip-ephem-wheel-cache-yssilu_k/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\nSuccessfully built transformers\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.42.3\n    Uninstalling transformers-4.42.3:\n      Successfully uninstalled transformers-4.42.3\nSuccessfully installed transformers-4.45.0.dev0\ntokenizers                               0.19.1\ntransformers                             4.45.0.dev0\n","output_type":"stream"}]},{"cell_type":"code","source":"notebook_login() #enter your token","metadata":{"execution":{"iopub.status.busy":"2024-08-10T17:01:46.865552Z","iopub.execute_input":"2024-08-10T17:01:46.866008Z","iopub.status.idle":"2024-08-10T17:01:46.897472Z","shell.execute_reply.started":"2024-08-10T17:01:46.86597Z","shell.execute_reply":"2024-08-10T17:01:46.896072Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1899d8f4c98e4bfbae3d94d545de1369"}},"metadata":{}}]},{"cell_type":"code","source":"import datasets\nfrom tokenizers import normalizers, pre_tokenizers, Tokenizer, models, trainers\nfrom datasets import load_dataset\n\n# Initialize a dataset\ndataset = load_dataset (\"breadlicker45/youtube-comments-v2\", split = 'train')","metadata":{"execution":{"iopub.status.busy":"2024-08-10T18:46:17.658551Z","iopub.execute_input":"2024-08-10T18:46:17.659228Z","iopub.status.idle":"2024-08-10T18:46:22.500368Z","shell.execute_reply.started":"2024-08-10T18:46:17.659164Z","shell.execute_reply":"2024-08-10T18:46:22.499187Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/24.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40fbd21861144074ada74bb74eef24ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/375685 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f99eb051c67469f860f664d90c093d5"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load pre-trained BPE tokenizer (e.g., GPT-2 tokenizer)\ntokenizer = AutoTokenizer.from_pretrained('gpt2')\n\n# Define a function to tokenize the text\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding='max_length', truncation=True, add_special_tokens({'pad_token': '[PAD]'}))\n\n# Apply the tokenizer to the dataset\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\n\n# Example of a tokenized entry\nprint(tokenized_dataset[0])","metadata":{"execution":{"iopub.status.busy":"2024-08-10T18:47:13.697008Z","iopub.execute_input":"2024-08-10T18:47:13.697476Z","iopub.status.idle":"2024-08-10T18:47:13.707291Z","shell.execute_reply.started":"2024-08-10T18:47:13.69744Z","shell.execute_reply":"2024-08-10T18:47:13.705328Z"},"trusted":true},"execution_count":30,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[30], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    return tokenizer(examples['text'], padding='max_length', truncation=True, add_special_tokens({'pad_token': '[PAD]'}))\u001b[0m\n\u001b[0m                                                                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"],"ename":"SyntaxError","evalue":"positional argument follows keyword argument (1940285355.py, line 8)","output_type":"error"}]}]}